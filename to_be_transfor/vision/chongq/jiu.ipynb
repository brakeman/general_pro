{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.loader \n",
    "    1.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import ipdb\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "annotation_path = \"../chongq/chongqing1_round1_train1_20191223/annotations.json\"\n",
    "img_path = './chongqing1_round1_train1_20191223/images/'\n",
    "\n",
    "\n",
    "class Jiu2(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "        with open(annotation_path, 'r') as load_f:\n",
    "            self.load_dict = json.load(load_f)\n",
    "            \n",
    "        self.tmp_img_dic = {}\n",
    "        for dic in self.load_dict['images']:\n",
    "            idx = dic['id']\n",
    "            self.tmp_img_dic[idx] = dic\n",
    "            \n",
    "        self.id2cls = {}\n",
    "        for dic in self.load_dict['categories']:\n",
    "            self.id2cls[dic['id']] = dic['name']\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.load_dict['annotations'][:100])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # img, mask, C, bbox, h, w\n",
    "        anno_dic = self.load_dict['annotations'][:100][index]\n",
    "        img_id = anno_dic['image_id']\n",
    "        H, W, C = self.tmp_img_dic[img_id]['height'], self.tmp_img_dic[img_id]['width'], anno_dic['category_id']\n",
    "        mask_img = self.mask_from_bbox(W, H, C, bbox=anno_dic['bbox'])\n",
    "        img_name = self.tmp_img_dic[img_id]['file_name']\n",
    "        img = cv2.imread(img_path+img_name).astype(np.float32)/255\n",
    "        return torch.from_numpy(img).permute([2,0,1]), torch.from_numpy(mask_img), C, torch.tensor(anno_dic['bbox']), H, W\n",
    "    \n",
    "    def mask_from_bbox(self, W, H, C, bbox):\n",
    "        # 图片的x, y轴与np的 x,y轴不一致；需要交换坐标系x,y才能互相转换；\n",
    "        mask = np.zeros((1, H, W))\n",
    "        x, y, w, h = [round(i) for i in bbox]\n",
    "        mask[0, y:y+h, x:x+w] = C\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    data = Jiu2()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Unet\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "train_loader = DataLoader(\n",
    "                    data,\n",
    "                    shuffle=RandomSampler(data),\n",
    "                    batch_size=10,\n",
    "                    num_workers=8,\n",
    "                    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /Users/bruce/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n",
      " 29%|██▉       | 24.3M/83.3M [43:49<1:46:21, 9.69kB/s] \n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "unpickling stack underflow",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ab0a7e1cee3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/general_pro/to_be_transfor/vision/chongq/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m         '''\n\u001b[1;32m    180\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet34\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatial_Channel_SE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/qb_env_2/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mresnet34\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m     \"\"\"\n\u001b[1;32m    242\u001b[0m     return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n\u001b[0;32m--> 243\u001b[0;31m                    **kwargs)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/qb_env_2/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_resnet\u001b[0;34m(arch, block, layers, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         state_dict = load_state_dict_from_url(model_urls[arch],\n\u001b[0;32m--> 217\u001b[0;31m                                               progress=progress)\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/qb_env_2/lib/python3.6/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash)\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mcached_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextraced_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/qb_env_2/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/qb_env_2/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: unpickling stack underflow"
     ]
    }
   ],
   "source": [
    "model = Unet()\n",
    "model.train()\n",
    "i = 0\n",
    "for imgs, masks, cls, bbox, H, W in train_loader:\n",
    "    if i == 0:\n",
    "        logits = model(imgs)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model):\n",
    "    running_loss = 0.0\n",
    "    data_size = train_data.__len__()\n",
    "\n",
    "    model.train()\n",
    "    # for inputs, masks, labels in progress_bar(train_loader, parent=mb):\n",
    "    for inputs, masks, labels in train_loader:\n",
    "        inputs, masks, labels = inputs.to(device), masks.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            if args.is_pseudo:\n",
    "                logit, logit_pixel, logit_image = model(inputs)\n",
    "                loss1 = lovasz_hinge(logit.squeeze(1), masks.squeeze(1))\n",
    "                loss2 = nn.BCELoss()(logit_image, labels)\n",
    "                loss3 = lovasz_hinge2(logit_pixel.squeeze(1), masks.squeeze(1))\n",
    "                loss = loss1 + loss2 + loss3\n",
    "            else:\n",
    "                logit = model(inputs)\n",
    "                loss = lovasz_hinge(logit.squeeze(1), masks.squeeze(1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        # mb.child.comment = 'loss: {}'.format(loss.item())\n",
    "    epoch_loss = running_loss / data_size\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.143\n",
      "0.27\n",
      "0.0769090909090909\n"
     ]
    }
   ],
   "source": [
    "from data import *\n",
    "def iou_thresh(w, h):\n",
    "    '''\n",
    "    iou_thresh(120, 400)\n",
    "    '''\n",
    "    min_axis = min(w, h)\n",
    "    if min_axis < 40:\n",
    "        val = 0.2\n",
    "    elif (min_axis >= 40) & (min_axis<120):\n",
    "        val = min_axis/200\n",
    "    elif (min_axis >= 120) & (min_axis<420):\n",
    "        val = min_axis/1500 + 0.52\n",
    "    elif min_axis>=420:\n",
    "        val = 0.8\n",
    "    else:\n",
    "        raise Exception('min_axis:{} not allowed'.format(min_axis))\n",
    "    return val\n",
    "\n",
    "\n",
    "def calcIOU(bbox1, bbox2):\n",
    "    '''\n",
    "    res  = calcIOU((1, 2, 2, 2), (2, 1, 2, 2))\n",
    "    '''\n",
    "    x1,y1,w1,h1 = bbox1\n",
    "    x2,y2,w2,h2 = bbox2\n",
    "    if((abs(x1 - x2) < ((w1 + w2)/ 2.0)) and (abs(y1-y2) < ((h1 + h2)/2.0))):\n",
    "        left = max((x1 - (w1 / 2.0)), (x2 - (w2 / 2.0)))\n",
    "        upper = max((y1 - (h1 / 2.0)), (y2 - (h2 / 2.0)))\n",
    "\n",
    "        right = min((x1 + (w1 / 2.0)), (x2 + (w2 / 2.0)))\n",
    "        bottom = min((y1 + (h1 / 2.0)), (y2 + (h2 / 2.0)))\n",
    "\n",
    "        inter_w = abs(left - right)\n",
    "        inter_h = abs(upper - bottom)\n",
    "        inter_square = inter_w * inter_h\n",
    "        union_square = (w1 * h1)+(w2 * h2)-inter_square\n",
    "\n",
    "        calcIOU = inter_square/union_square * 1.0\n",
    "        return round(calcIOU, 3) \n",
    "    else:\n",
    "        return 0\n",
    "    return calcIOU\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def cal_ap(c_pred, c_label, pred_bbox, real_bbox, img_w, img_h):\n",
    "    '''\n",
    "    c_pred: [bs];\n",
    "    c_label: int;\n",
    "    pred_bbox: list of [x,y,w,h];\n",
    "    real_bbox: list of [x,y,w,h];\n",
    "    img_w: [w]\n",
    "    img_h: [h]\n",
    "    '''\n",
    "    \n",
    "    round1_idx = np.where(c_pred == c_label)[0]\n",
    "#     ipdb.set_trace()\n",
    "    pred_list = []\n",
    "    for idx in round1_idx:\n",
    "        iou_th = iou_thresh(img_w[idx],img_h[idx])\n",
    "        is_true = (calcIOU(pred_bbox[idx], real_bbox[idx]) > iou_th) * 1.0\n",
    "        pred_list.append(is_true)\n",
    "#     ipdb.set_trace()\n",
    "    round2_idx = np.where(np.array(pred_list) == 1.0)[0]\n",
    "    return len(round2_idx)/len(c_pred)\n",
    "\n",
    "\n",
    "def mAP(weight, c_pred, c_label, pred_bbox, real_bbox, img_w, img_h):\n",
    "    '''\n",
    "    weight: {C: weight_value};\n",
    "    c_pred: [bs];\n",
    "    c_label: [bs];\n",
    "    pred_bbox: list of [x,y,w,h];\n",
    "    real_bbox: list of [x,y,w,h];\n",
    "    img_w: [w]\n",
    "    img_h: [h]\n",
    "    '''\n",
    "    mAp = 0\n",
    "    c_label = np.array(c_label)\n",
    "    for C in range(1,10):\n",
    "        C_idx = np.where(c_label==C)[0]\n",
    "#         ipdb.set_trace()\n",
    "        \n",
    "        C_ap = cal_ap(np.array(c_pred)[C_idx], np.array(c_label)[C_idx], np.array(pred_bbox)[C_idx], \n",
    "                                np.array(real_bbox)[C_idx], np.array(img_w)[C_idx], np.array(img_h)[C_idx])\n",
    "        mAp+=C_ap*weight[C]\n",
    "    return mAp\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # iou\n",
    "    res  = calcIOU((1, 2, 2, 2), (2, 1, 2, 2))\n",
    "    print(res)\n",
    "    \n",
    "    \n",
    "    # ap & mAP\n",
    "    c_label_list = np.random.randint(1,11,100)\n",
    "    c_label = 2\n",
    "    c_pred = np.random.randint(0,4,100)\n",
    "    data = Jiu()\n",
    "    bb1_list = []\n",
    "    for i in range(100):\n",
    "        bb1_list.append(data[i][4])\n",
    "    bb2_list = bb1_list\n",
    "    img_w = np.random.randint(20,450,100)\n",
    "    img_h = np.random.randint(20,450,100)\n",
    "    ap = cal_ap(c_pred, c_label, bb1_list, bb1_list, img_w, img_h)\n",
    "    print(ap)\n",
    "    # weight\n",
    "    weight_dic = {1:0.15,\n",
    "    2:0.09,\n",
    "    3:0.09,\n",
    "    4:0.05,\n",
    "    5:0.13,\n",
    "    6:0.05,\n",
    "    7:0.12,\n",
    "    8:0.13,\n",
    "    9:0.07,\n",
    "    10:0.12}\n",
    "    map_ = mAP(weight_dic, c_pred, c_label_list, bb1_list, bb1_list, img_w, img_h)\n",
    "    print(map_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
