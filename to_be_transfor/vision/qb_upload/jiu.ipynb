{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标是先建立一个非常强的baseline；数据端先简易来；模型端好好弄，loss好好弄， eval好好弄;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fake_Spatial_SE(nn.Module):\n",
    "    '''\n",
    "    conv2d(channel_in, channel_out, kernel=(1,1))\n",
    "        # H_new = [(H_old+padding-1)/stride]+1\n",
    "        # 这一层卷积操作会是的原始feature map[bs, channel_in, H, W] --> [bs, channel_out, H, W]\n",
    "    '''\n",
    "    def __init__(self, channel, num_cls=11):\n",
    "        super(Fake_Spatial_SE, self).__init__()\n",
    "        self.squeeze = nn.Conv2d(channel, num_cls, kernel_size=1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "#         ipdb.set_trace()\n",
    "        z = self.squeeze(x) # bs, num_cls, H, W\n",
    "        z = self.sigmoid(z) # bs, num_cls, H, W\n",
    "        return z \n",
    "    \n",
    "    \n",
    "class FakeDecoder(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out=11):\n",
    "        '''\n",
    "        up_in: channel of skip conneted layer; \n",
    "        x_in: channel of last layer;\n",
    "        n_out: channel of output;\n",
    "        '''\n",
    "        super(FakeDecoder, self).__init__()\n",
    "        self.x_conv = nn.Conv2d(x_in, n_out, 1, bias=False)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, n_out, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.fake_sSE = Fake_Spatial_SE(channel=n_out)\n",
    "\n",
    "    def forward(self, up_p, x_p):\n",
    "#         ipdb.set_trace()\n",
    "        up_p = self.tr_conv(up_p) # [bs, 256, 8, 8] --> [bs, num_cls, 16, 16]\n",
    "        fake_sSE = self.fake_sSE(up_p) # [bs, num_cls, 16, 16]\n",
    "        x_p = self.x_conv(x_p) # [bs, 11, 16, 16] -->  [bs, num_cls, 16, 16]\n",
    "        return x_p*fake_sSE # [bs, 11, 16, 16]\n",
    "    \n",
    "\n",
    "\n",
    "class Unet_qb(nn.Module):\n",
    "    def __init__(self, num_class=11):\n",
    "        '''\n",
    "        up_in: channel of skip conneted layer; \n",
    "        x_in: channel of last layer;\n",
    "        n_out: channel of output;\n",
    "        '''\n",
    "        super(Unet_qb, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet34(True)\n",
    "        self.conv1 = nn.Sequential( self.resnet.conv1, self.resnet.bn1, self.resnet.relu)\n",
    "        self.encode2 = nn.Sequential(self.resnet.layer1, Spatial_Channel_SE(64))\n",
    "        self.encode3 = nn.Sequential(self.resnet.layer2, Spatial_Channel_SE(128))\n",
    "        self.encode4 = nn.Sequential(self.resnet.layer3, Spatial_Channel_SE(256))\n",
    "        self.encode5 = nn.Sequential(self.resnet.layer4, Spatial_Channel_SE(512))\n",
    "        self.center = nn.Sequential(PyramidAttention(512, 256), nn.MaxPool2d(2, 2)) \n",
    "        self.decode5 = FakeDecoder(256, 512, num_class)\n",
    "        self.decode4 = FakeDecoder(64, 256, num_class)\n",
    "        self.decode3 = FakeDecoder(64, 128, num_class)\n",
    "        self.decode2 = FakeDecoder(64, 64, num_class)\n",
    "        self.decode1 = FakeDecoder(64, 32, num_class)\n",
    "        self.logit = nn.Sequential(nn.Conv2d(320, 64, kernel_size=3, padding=1),\n",
    "                                   nn.ELU(True),\n",
    "                                   nn.Conv2d(64, num_class, kernel_size=1, bias=False))\n",
    "        self.logit_image = nn.Linear(256, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 3, 256, 256)\n",
    "        x = self.conv1(x)  # 64, 128, 128\n",
    "        e2 = self.encode2(x)  # 64, 128, 128\n",
    "        e3 = self.encode3(e2)  # 128, 64, 64\n",
    "        e4 = self.encode4(e3)  # 256, 32, 32\n",
    "        e5 = self.encode5(e4)  # 512, 16, 16\n",
    "        f = self.center(e5)  # 256, 8, 8\n",
    "        for_cls = F.adaptive_avg_pool2d(f, output_size=1) # 256\n",
    "        d5 = self.decode5(f, e5)  # num_cls, 16, 16\n",
    "        d4 = self.decode4(d5, e4)  # num_cls, 32, 32\n",
    "        d3 = self.decode3(d4, e3)  # num_cls, 64, 64\n",
    "        d2 = self.decode2(d3, e2)  # num_cls, 128, 128\n",
    "        d1 = self.decode1(d2, x)  # num_cls, 256, 256\n",
    "        f = torch.cat((d1,\n",
    "                       F.upsample(d2, scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                       F.upsample(d3, scale_factor=4, mode='bilinear', align_corners=True),\n",
    "                       F.upsample(d4, scale_factor=8, mode='bilinear', align_corners=True),\n",
    "                       F.upsample(d5, scale_factor=16, mode='bilinear', align_corners=True)), 1)  # 320, 256, 256\n",
    "\n",
    "        logit = self.logit(f)  # 11, 256, 256\n",
    "        clf = self.logit_image(for_cls.view(-1, 256)) # bs, 11\n",
    "        return logit, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch acc\n",
    "\n",
    "def valid_look(fold_id, pth=None):\n",
    "    # 由于 img_ori shape 不一致，所以无法放到 loader 里以 batch的形式呈现，因此，bs必须等于1；\n",
    "    # 之所以要 img_ori, 是因为想要画图展示，想要计算ori_bbox\n",
    "\n",
    "    if pth is None:\n",
    "        param = torch.load('./models/unet_49' + '.pth')  # stage3 use model pretrained with pseudo-labels\n",
    "        model.load_state_dict(param)  # initialize with pretained weight\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    valid_data_tmp = Jiu_valid(fold_id=fold_id, mode='valid')\n",
    "    loader = DataLoader(valid_data_tmp,\n",
    "                            batch_size=1,\n",
    "                            num_workers=8,\n",
    "                            pin_memory=True)\n",
    "    i = 0\n",
    "    c_pred, c_label_list, bb1_list, bb1_list, img_w, img_h = [],[],[],[],[],[]\n",
    "    for imgs, mask, C, bbox, H, W, img_ori in progress_bar(loader):   \n",
    "        i+=1\n",
    "        if i == 1:\n",
    "            imgs = imgs.to(device)\n",
    "            with torch.no_grad():\n",
    "                mask_pred = model(imgs).to('cpu').softmax(1).argmax(1)[0] # w, h\n",
    "                bbox_pred = mask2bbox_withscale(mask_pred.numpy(), H, W)\n",
    "                img = img_ori[0].numpy() # h,w,3\n",
    "                x, y, w, h = bbox.numpy()[0]\n",
    "                x1, y1, w1, h1 = [i.tolist()[0] for i in bbox_pred]\n",
    "                c_label_list.append(C)\n",
    "                bb1_list.append([x, y, w, h])\n",
    "                bb2_list.append([x1, y1, w1, h1])\n",
    "                img_w.append(W)\n",
    "                img_h.append(H)\n",
    "                if 1:\n",
    "                    cv2.rectangle(img, (x, y), (x + w, y + h), (1, 0, 0), 2)\n",
    "                    cv2.rectangle(img, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 1), 2)\n",
    "                    vis.image(img_gt.transpose((2,0,1)), win='valid1', opts={'title':'valid_gt_{}'.format(C)})\n",
    "                    vis.image(img_pred.transpose((2,0,1)), win='valid1', opts={'title':'img_pred'})\n",
    "        else:\n",
    "            break\n",
    "        weight_dic = {1:0.15,\n",
    "                    2:0.09,\n",
    "                    3:0.09,\n",
    "                    4:0.05,\n",
    "                    5:0.13,\n",
    "                    6:0.05,\n",
    "                    7:0.12,\n",
    "                    8:0.13,\n",
    "                    9:0.07,\n",
    "                    10:0.12}\n",
    "    mAp = mAP(weight_dic, c_pred, c_label_list, bb1_list, bb1_list, img_w, img_h)\n",
    "    return mAp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import ipdb\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from data import JiuData, do_resize, JiuTest\n",
    "from model import *\n",
    "from augment import *\n",
    "from loss import multi_class_entropy, lovasz_softmax\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from eval import mAP, mask2bbox_withscale, mask2bbox\n",
    "from timeit import default_timer as timer\n",
    "from visdom import Visdom\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ipdb\n",
    "\n",
    "def time_to_str(t, mode='min'):\n",
    "    from timeit import default_timer as timer\n",
    "    \n",
    "    if mode=='min':\n",
    "        t  = int(t)/60\n",
    "        hr = t//60\n",
    "        min = t%60\n",
    "        return '%2d hr %02d min'%(hr,min)\n",
    "    elif mode=='sec':\n",
    "        t   = int(t)\n",
    "        min = t//60\n",
    "        sec = t%60\n",
    "        return '%2d min %02d sec'%(min,sec)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def train(loader, model, dataset, device):\n",
    "    model = model.to(device)\n",
    "    running_loss = 0.\n",
    "    model.train()\n",
    "    for imgs, masks, cls, bbox, H, W in progress_bar(loader, parent=mb):\n",
    "        optimizer.zero_grad()\n",
    "#         ipdb.set_trace()\n",
    "        imgs, masks, cls = imgs.to(device), masks.to(device), cls.to(device)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            logits, cls_logits = model(imgs) # [bs, cls, H, W] [bs, cls]\n",
    "            loss1 = multi_class_entropy(logits, masks.squeeze().int())\n",
    "            loss2 = lovasz_softmax(logits.squeeze(), masks.squeeze().int(), per_image=False)\n",
    "            loss3 = torch.nn.CrossEntropyLoss()(cls_logits, cls)\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss.item()*imgs.size(0)\n",
    "    return running_loss/len(dataset)\n",
    "\n",
    "\n",
    "def valid(loader, model, dataset, device):\n",
    "    model = model.to(device)\n",
    "    run_loss, run_loss1, run_loss2, run_loss3 = 0, 0,0,0\n",
    "    model.eval()\n",
    "    for imgs, masks, cls, bbox, H, W in progress_bar(loader, parent=mb):    \n",
    "        imgs, masks, cls = imgs.to(device), masks.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits, cls_logits = model(imgs)\n",
    "            loss1 = multi_class_entropy(logits, masks.squeeze().int())\n",
    "            loss2 = lovasz_softmax(logits.squeeze(), masks.squeeze().int(), per_image=False)\n",
    "            loss3 = torch.nn.CrossEntropyLoss()(cls_logits, cls)\n",
    "            loss = loss1 + loss2 + loss3\n",
    "        run_loss += loss.item()*imgs.size(0)\n",
    "        run_loss1 += loss1.item()*imgs.size(0)\n",
    "        run_loss2 += loss2.item()*imgs.size(0)\n",
    "        run_loss3 += loss3.item()*imgs.size(0)\n",
    "    return run_loss/len(dataset), run_loss1/len(dataset), run_loss2/len(dataset), run_loss3/len(dataset)\n",
    "\n",
    "\n",
    "def valid_look(model, fold_id, show_num=3, pth=1):\n",
    "    # logit # bs, 11, h, w \n",
    "    # mask_pred\n",
    "    # bbox_pred\n",
    "    # picture\n",
    "    if pth is None:\n",
    "        param = torch.load('./models/unet_49' + '.pth')  # stage3 use model pretrained with pseudo-labels\n",
    "        model.load_state_dict(param)  # initialize with pretained weight\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    valid_data_tmp = JiuData(fold_id=fold_id, mode='valid', return_ori_img=True)\n",
    "    loader = DataLoader(valid_data_tmp,\n",
    "                        sampler = RandomSampler(valid_data_tmp),\n",
    "                        batch_size=1,\n",
    "                        num_workers=8,\n",
    "                        pin_memory=True)\n",
    "    i = 0\n",
    "    for imgs, mask, C, bbox, H, W, img_ori in loader:   \n",
    "        i+=1\n",
    "        if i <= show_num:\n",
    "            imgs = imgs.to(device)\n",
    "            with torch.no_grad():\n",
    "                mask_pred, cls_logit = model(imgs) # w, h\n",
    "                mask_pred = mask_pred.to('cpu').softmax(1).argmax(1)[0]\n",
    "                bbox_pred = mask2bbox_withscale(mask_pred.numpy(), H, W)\n",
    "                img  = img_ori[0].numpy() # h,w,3\n",
    "                img_2 = deepcopy(img)\n",
    "                x, y, w, h = bbox.numpy()[0]\n",
    "                x1, y1, w1, h1 = [i.tolist()[0] for i in bbox_pred]\n",
    "                print('mask pred rectangle: {}/{}'.format((x1, y1), (x1 + w1, y1 + h1)))\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (1, 0, 0), 2)\n",
    "                cv2.rectangle(img_2, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 1), 2)\n",
    "                vis.image(img.transpose((2,0,1)), win='valid_gt', update='append', opts={'title':'valid_gt_{}'.format(C)})\n",
    "                vis.image(img_2.transpose((2,0,1)), win='valid_pred', update='append', opts={'title2':'valid_pred_{}'.format(C)})\n",
    "        else:\n",
    "            break\n",
    "    return \n",
    "\n",
    "\n",
    "def infer_upload(pre_train_pth=None):\n",
    "    if pre_train_pth is None:\n",
    "        param = torch.load('./models/unet_49' + '.pth')  # stage3 use model pretrained with pseudo-labels\n",
    "        model.load_state_dict(param)  # initialize with pretained weight\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    dic = {}\n",
    "    dic['images'] = []\n",
    "    dic['annotations']=[]\n",
    "    for index, tup in enumerate(progress_bar(test_loader)):  \n",
    "        name, imgs, ori_h, ori_w = tup\n",
    "        dic['images'].append({'file_name': name,\n",
    "                              'id': index+1})\n",
    "\n",
    "        imgs = imgs.to(device)\n",
    "        with torch.no_grad():\n",
    "            mask = model(imgs).to('cpu').softmax(1).argmax(1)[0] #w,h\n",
    "            bbox_pred = mask2bbox_withscale(mask.numpy(), ori_h, ori_w)\n",
    "            dic['annotations'].append({'image_id':index+1,\n",
    "                           'bbox':bbox_pred,\n",
    "                           'category_id':mask.max(),\n",
    "                           'score':1})\n",
    "    return dic\n",
    "\n",
    "\n",
    "weight_dic = {1:0.15,\n",
    "2:0.09,\n",
    "3:0.09,\n",
    "4:0.05,\n",
    "5:0.13,\n",
    "6:0.05,\n",
    "7:0.12,\n",
    "8:0.13,\n",
    "9:0.07,\n",
    "10:0.12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "model = Unet()\n",
    "lr = 0.01\n",
    "min_lr = 0.00015\n",
    "EPOCH=200\n",
    "snapshot = 4\n",
    "scheduler_step = EPOCH//snapshot\n",
    "device = 'cuda'\n",
    "fold_id = 1\n",
    "\n",
    "train_data = JiuData(fold_id=fold_id, mode='train', return_ori_img=False)\n",
    "train_loader = DataLoader(\n",
    "                    train_data,\n",
    "                    shuffle=RandomSampler(train_data),\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=8,\n",
    "                    pin_memory=True)\n",
    "\n",
    "valid_data = JiuData(fold_id=fold_id, mode='valid', return_ori_img=False)\n",
    "valid_loader = DataLoader(\n",
    "                    valid_data,\n",
    "                    shuffle=RandomSampler(valid_data),\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=8,\n",
    "                    pin_memory=True)\n",
    "\n",
    "\n",
    "testset = JiuTest()\n",
    "test_loader = DataLoader(\n",
    "                    testset,\n",
    "                    batch_size=1,\n",
    "                    num_workers=8,\n",
    "                    pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m visdom.server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n",
      "Setting up a new session...\n",
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "vis = Visdom(env='jiujiu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='200', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/200 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='70' class='' max='70', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [70/70 00:17<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='200', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/200 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='70' class='' max='70', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [70/70 00:17<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'deepcopy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6aea6c2dafbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ml_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalid_look\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d3ef1210a0b3>\u001b[0m in \u001b[0;36mvalid_look\u001b[0;34m(model, fold_id, show_num, pth)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mbbox_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask2bbox_withscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mimg\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mimg_ori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# h,w,3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mimg_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbbox_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deepcopy' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'deepcopy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6aea6c2dafbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ml_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalid_look\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d3ef1210a0b3>\u001b[0m in \u001b[0;36mvalid_look\u001b[0;34m(model, fold_id, show_num, pth)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mbbox_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask2bbox_withscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mimg\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mimg_ori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# h,w,3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mimg_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbbox_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deepcopy' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'deepcopy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6aea6c2dafbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ml_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalid_look\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshowlegend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'append'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-d3ef1210a0b3>\u001b[0m in \u001b[0;36mvalid_look\u001b[0;34m(model, fold_id, show_num, pth)\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mbbox_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask2bbox_withscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mimg\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mimg_ori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# h,w,3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mimg_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbbox_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deepcopy' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(params = model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=scheduler_step, eta_min=min_lr)\n",
    "start = timer()\n",
    "mb = master_bar(range(EPOCH))\n",
    "\n",
    "for epoch in mb:\n",
    "    scheduler.step()\n",
    "    epoch_loss = train(train_loader, model, train_data, device)\n",
    "    l_all, l1, l2, l3 = valid(valid_loader, model, valid_data, device)\n",
    "    valid_look(model, fold_id)\n",
    "    vis.line(X=[epoch], Y=[[epoch_loss, l_all, l1, l2, l3]], opts=dict(markers=True, showlegend=True), win='loss', update='append' if epoch>0 else None)\n",
    "    \n",
    "    # 重置lr;\n",
    "    if (epoch + 1) % scheduler_step == 0:\n",
    "        torch.save(model.state_dict(),  './models/unet_aug01_'+ str(epoch) + '.pth')\n",
    "        optimizer = torch.optim.SGD(params = model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=scheduler_step, eta_min=min_lr)\n",
    "        scheduler.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
