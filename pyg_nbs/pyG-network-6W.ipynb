{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/qibo/all_project/Graph反欺诈/PYG'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given \n",
    "feat.json(ctx给的feat.npy文件转化的) && ctx_joined_table_phone.csv(loanid - phone runlist) \n",
    "1. 我先通过 to_temp_file 函数，把符合窗口的 node_id 拿到\n",
    "2. 跑query ctx_... 拿到符合条件的边 和 id2phone（损失一部分source 点，因为这些phone没有注册过（未知bug））-- edge.json 【id2phone.json】&&【edge 文件draft】\n",
    "3. 通过符合条件的边 unique, 去找到 node_feat (损失一部分点， bug;没找出来原因; 这些phone 在node_feat.json 中不存在) 【feat 文件】\n",
    "4. 删除edge.json 中 作为source node 缺不在 node_feat 字典中存在的元素；【edge 文件final】\n",
    "5. 同上，重写【phone2id.json文件】\n",
    "6. 扩充； 尽管大部分edge 的端点都是\n",
    "\n",
    "6. 一个事实是， 尽管source node 全部都有 feat, 但是大量的 target nodes 是没feat vec 的；我的方案是0向量；但是不能读入内存，内存爆炸了，改相应的源码吧，不预先读了；；\n",
    "7. 既然孤点无增益，那么删掉孤点；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "孤点如何定义？\n",
    "networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import requests\n",
    "from urllib.parse import urljoin \n",
    "import time \n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "GRAPHSQL_URL = \"http://192.168.20.241:9000/query/OneMonthNet/\"\n",
    "\n",
    "def query_graphsql(query_name, para_string):\n",
    "    remaining_url = \"{}?{}\".format(query_name, para_string)\n",
    "    url=urljoin(GRAPHSQL_URL, remaining_url)\n",
    "    st = time.time()\n",
    "    print('query:{}\\nparas:{}\\nrequest for url: {}'.format(query_name, para_string.split('&'), url))\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "    try:\n",
    "        result = requests.get(url)\n",
    "        result_json = json.loads(result.text)\n",
    "        if result_json['error']:\n",
    "            logging.error(result_json['message'])\n",
    "            print('run query failed')\n",
    "            return None\n",
    "        print('run query finish, use {} seconds\\n\\n'.format(time.time() - st))\n",
    "        return result_json\n",
    "    except Exception as e:\n",
    "        print('failed')\n",
    "        \n",
    "def ctx_get_phone_neighs_num_with_file(file_path, thresh):\n",
    "    query_name='get_phone_neighs_num_with_file'\n",
    "    paras = 'file_={}&thresh={}'.format(file_path, thresh)\n",
    "    return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "def node_neigh_show(v_id, v_type):\n",
    "    '''\n",
    "    '''\n",
    "    query_name='node_neigh_show'\n",
    "    paras = 'seed={}&seed.type={}'.format(v_id, v_type) \n",
    "    return query_graphsql(query_name, paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60000 src & 百万unique\n",
    "# 百万unique  虎图> 1000\n",
    "# 百万unique 只剩 比如90W , 因为删掉了（10086 且 没在60000src中的点） 大致操作 就是把原边文件逐个遍历，只要在to_del_list中，就删掉\n",
    "# 90W 变成边 nx cc \n",
    "# 删掉孤点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60000 src \n",
    "with open('/data-0/tigergraph/30_W_samples/ctx_neighs_phone.json', 'r') as fp:\n",
    "    ctx_edges = json.load(fp)\n",
    "temp_df = pd.DataFrame.from_dict(ctx_edges['results'][0]['vv'])\n",
    "temp_df['num_neighs'] = temp_df.attributes.apply(lambda x: len(x['vv.@node_neighs']))\n",
    "\n",
    "\n",
    "# 百万unique;\n",
    "UNIQUE = []\n",
    "for dic in tqdm(ctx_edges['results'][0]['vv']):\n",
    "    len_ = len(dic['attributes']['vv.@node_neighs'])\n",
    "    new_src = dic['v_id']\n",
    "    src_neighs = dic['attributes']['vv.@node_neighs']\n",
    "    src_neighs.extend(new_src)\n",
    "    UNIQUE.extend(src_neighs)\n",
    "    \n",
    "# 虎图 > thresh （百万unique仅有23545符合）;\n",
    "tiger_thresh = 1500\n",
    "tiger_phone_nei_num = ctx_get_phone_neighs_num_with_file('/home/qibo/all_project/Graph反欺诈/PYG/unique_phones_6W.csv' , tiger_thresh)\n",
    "temp_phone_neighs = pd.DataFrame.from_dict(tiger_phone_nei_num['results'][0]['vv'])\n",
    "temp_phone_neighs['num_neighs'] = temp_phone_neighs.attributes.apply(lambda x: x['vv.@node_neighs'])\n",
    "\n",
    "# 90W 变成边\n",
    "to_delete = temp_phone_neighs.v_id.tolist()\n",
    "src_6w = temp_df.v_id.tolist()\n",
    "real2delete = [i for i in to_delete if i not in src_6w]\n",
    "EDGES = []\n",
    "set_t = set(real2delete)\n",
    "for dic in tqdm(ctx_edges['results'][0]['vv']):\n",
    "    if dic['v_id'] not in real2delete:\n",
    "        set_temp = set([i for i in dic['attributes']['vv.@node_neighs'] if len(i) >5])\n",
    "        remain_set = set_temp - set_t # 只留下neighs 中不属于 real2delte 的部分；\n",
    "        len_ = len(remain_set)\n",
    "        new_src = [dic['v_id']]*len_\n",
    "        src_neighs = list(remain_set)\n",
    "        EDGES.extend(list(zip(new_src, src_neighs)))\n",
    "\n",
    "# nx & delete single;\n",
    "# 通过nx 找到cc，删除这个子图的孤点； \n",
    "G = nx.Graph()   # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
    "G.add_edges_from(EDGES) # using a list of edge tuples\n",
    "res = list(nx.connected_components(G))\n",
    "\n",
    "## 卡住，因为这里只有一个子图；bug在于node 中居然有 0，1，2，3，4，5，6，。。 改了改上面代码，删掉电话号下雨等于5位的；\n",
    "## 卡住2， 因为第1大的子图，还是千万级别的； 中心度最高的是真人， 其打过的电话中6000多个点在虎图上作为phoneNumber； 他是电话业务员；\n",
    "## 对于pyG 来说，卡住于此了; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 想删除中心度太大的点\n",
    "deg_cent_dic = nx.degree_centrality(G)\n",
    "deg_df = pd.DataFrame.from_dict(deg_cent_dic, orient='index')\n",
    "deg_df.columns = ['deg']\n",
    "deg_df = deg_df.sort_values('deg', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:node_neigh_show\n",
      "paras:['seed=17716795256', 'seed.type=PhoneNumber']\n",
      "request for url: http://192.168.20.241:9000/query/OneMonthNet/node_neigh_show?seed=17716795256&seed.type=PhoneNumber\n",
      "---------------------------------------------------------------------------------------------------\n",
      "run query finish, use 0.011165618896484375 seconds\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'version': {'api': 'v2', 'schema': 0},\n",
       " 'error': False,\n",
       " 'message': '',\n",
       " 'results': [{'vv': [{'v_id': '17716795256',\n",
       "     'v_type': 'PhoneNumber',\n",
       "     'attributes': {'vv.prim_id': '17716795256', 'vv.@node_neighs': 5713}}]}]}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_neigh_show('17716795256', 'PhoneNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> G = nx.path_graph(4)\n",
    ">>> G.add_path([10, 11, 12])\n",
    ">>> [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SplineConv\n",
    "\n",
    "dataset = 'Cora'\n",
    "\n",
    "dataset = Planetoid('/home/qibo/all_project/Graph反欺诈/', dataset, T.TargetIndegree())\n",
    "data = dataset[0]\n",
    "\n",
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
    "data.train_mask[:data.num_nodes - 1000] = 1\n",
    "data.val_mask = None\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
    "data.test_mask[data.num_nodes - 500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(4, 4)\n",
    "b = torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1.0112, 0.9588, 0.5494, 1.4970]),\n",
       "indices=tensor([1, 0, 0, 1]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.va"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SplineConv, GCNConv\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data-0/tigergraph/30W_2/feat.json',\n",
       " '/data-0/tigergraph/30W_2/x_phone2idx.json',\n",
       " '/data-0/tigergraph/30W_2/id2phone_dic.json',\n",
       " '/data-0/tigergraph/30W_2/phone2idx.json',\n",
       " '/data-0/tigergraph/30W_2/label.json',\n",
       " '/data-0/tigergraph/30W_2/edge.json']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_root = '/data-0/tigergraph/30W_2/'\n",
    "glob.glob(save_root+'/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cashbus_data(save_root):\n",
    "    ##################### finish add test infos into the graph ############################\n",
    "    with open(save_root+'/x_phone2idx.json') as json_file:\n",
    "        x_phone2idx = json.load(json_file)\n",
    "    with open(save_root+'/edge.json') as json_file:\n",
    "        edge = json.load(json_file)\n",
    "    with open(save_root+'/label.json') as json_file:\n",
    "        label = json.load(json_file)\n",
    "    with open(save_root+'/feat.json') as json_file:\n",
    "        x = json.load(json_file)\n",
    "\n",
    "    ##################### finish add test infos into the graph ############################\n",
    "    feat_mat = []\n",
    "    for k,v in tqdm(x.items()):\n",
    "        feat_mat.append(v)\n",
    "        \n",
    "    labels = list(label.values())\n",
    "    x = torch.tensor(feat_mat, dtype = torch.float)    \n",
    "    y = torch.tensor(labels, dtype=torch.int64)\n",
    "    \n",
    "    edge = np.array(edge).T\n",
    "    row1 = [x_phone2idx[str(i)] for i in edge[0]]\n",
    "    row2 = [x_phone2idx[str(i)] for i in edge[1]]\n",
    "    new_edges = torch.tensor(np.stack([row1, row2]))\n",
    "    \n",
    "    ##################### finish all ############################\n",
    "    data = Data(x=x, edge_index=new_edges, y=y)\n",
    "    return data\n",
    "\n",
    "class CashBus(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(CashBus, self).__init__(root, transform, pre_transform)\n",
    "        print('processed_path:{}'.format(self.processed_paths))\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['feat.json', 'x_phone2idx.json', 'edge.json', 'label.json']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data12.pt'\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        print('go pl, raw_dir:{}'.format(self.raw_dir))\n",
    "        data = read_cashbus_data(self.raw_dir)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path:['/home/qibo/all_project/Graph反欺诈/PYG/30W_data/processed/data12.pt']\n",
      "0.5365092754364014\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "root = '/home/qibo/all_project/Graph反欺诈/PYG/30W_data'\n",
    "dataset = CashBus(root)\n",
    "data = dataset[0]\n",
    "print(time.time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_mask = torch.zeros(len(data.y), dtype=torch.uint8)\n",
    "data.train_mask[:data.num_nodes - 1000] = 1\n",
    "data.val_mask = None\n",
    "data.test_mask = torch.zeros(len(data.y), dtype=torch.uint8)\n",
    "data.test_mask[data.num_nodes - 500:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3289057])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((data.edge_index.size(1),)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14127, 14487])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1200525 is out of bounds for dimension 0 with size 14127",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-164ba8341530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'use {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_test_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-164ba8341530>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/qb_vir_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-164ba8341530>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/qb_vir_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/qb_vir_env/lib/python3.6/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             edge_index, norm = self.norm(edge_index, x.size(0), edge_weight,\n\u001b[0;32m---> 89\u001b[0;31m                                          self.improved, x.dtype)\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/qb_vir_env/lib/python3.6/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(edge_index, num_nodes, edge_weight, improved, dtype)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0medge_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1200525 is out of bounds for dimension 0 with size 14127"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16, cached=True)\n",
    "        self.conv1_1 = GCNConv(16, 16, cached=True)\n",
    "        self.conv1_2 = GCNConv(16, 16, cached=True)\n",
    "        self.conv2 = GCNConv(16, int(dataset.num_classes), cached=True)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.conv1_1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.conv1_2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        print(data.y[mask].sum().item() / len(data.y[mask]))\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 2):\n",
    "    print('epoch:{}'.format(epoch))\n",
    "    st = time.time()\n",
    "    train()\n",
    "    print('use {} seconds'.format(time.time() - st))\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_acc, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
    "data.train_mask[:data.num_nodes - 1000] = 1\n",
    "data.val_mask = None\n",
    "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.uint8)\n",
    "data.test_mask[data.num_nodes - 500:] = 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def __get_mask_idx(labels_df, test_st='2019-06-03 00:00:00', test_et='2019-06-04 00:00:00',train_valid_split=0.8):\n",
    "    train_idx = labels_df[labels_df.fundtime<test_et].username.tolist()\n",
    "    test_idx = labels_df[labels_df.fundtime>=test_st].username.tolist()\n",
    "    split_idx = int(len(train_idx)*train_valid_split)\n",
    "    train = train_idx[:split_idx]\n",
    "    valid = train_idx[split_idx:]\n",
    "    print(len(train), len(valid), len(test_idx))\n",
    "    return train, valid, test_idx\n",
    "\n",
    "def _get_final_label(labels, new_x_phone2idx):\n",
    "    \n",
    "    label_dict = labels[['username', 'default_now']].set_index('username').to_dict()['default_now']\n",
    "    train_phone, valid_phone, test_phone = __get_mask_idx(labels)\n",
    "    fake_labels = np.zeros(len(new_x_phone2idx))-1\n",
    "\n",
    "    for phone in label_dict:\n",
    "        idx = new_x_phone2idx[str(phone)]\n",
    "        fake_labels[idx] = label_dict[int(phone)]   \n",
    "\n",
    "    train_mask = [new_x_phone2idx[str(phone)] for phone in train_phone]\n",
    "    fake_train_labels = np.zeros(len(new_x_phone2idx))\n",
    "    fake_train_labels[train_mask] = 1\n",
    "\n",
    "    valid_mask = [new_x_phone2idx[str(phone)] for phone in valid_phone]\n",
    "    fake_valid_labels = np.zeros(len(new_x_phone2idx))\n",
    "    fake_valid_labels[valid_mask] = 1\n",
    "    \n",
    "    test_mask = [new_x_phone2idx[str(phone)] for phone in test_phone]\n",
    "    fake_test_labels = np.zeros(len(new_x_phone2idx))\n",
    "    fake_test_labels[test_mask] = 1\n",
    "    \n",
    "    return fake_labels, fake_train_labels, fake_valid_labels, fake_test_labels\n",
    "\n",
    "\n",
    "def _add_new_feat(x, test_df):\n",
    "    not_in_edge_lis = []\n",
    "    for i in test_df.username:\n",
    "        if str(i) not in x:\n",
    "            x[str(i)] = [0, 0, 0, 0]\n",
    "            not_in_edge_lis.append(str(i))\n",
    "    return x, not_in_edge_lis\n",
    "\n",
    "\n",
    "def _add_new_edge(edge, test_df, not_in_edge_lis):\n",
    "    for i in test_df.username:\n",
    "        if str(i) in not_in_edge_lis:\n",
    "            edge.append([str(i), str(i)])\n",
    "    return edge\n",
    "    \n",
    "def _add_phone2ix(x_phone2idx, not_in_edge_lis):\n",
    "    old_len = len(x_phone2idx)\n",
    "    i=0\n",
    "    for new_phone in not_in_edge_lis:\n",
    "        x_phone2idx[new_phone] = old_len+i\n",
    "        i+=1\n",
    "    return x_phone2idx\n",
    "\n",
    "def get_all_info(x, edge, x_phone2idx, test_df):\n",
    "    print(len(x), len(edge), len(x_phone2idx))\n",
    "    print('--------------------------')\n",
    "    x, not_in_edge_lis = _add_new_feat(x, test_df)\n",
    "    edge = _add_new_edge(edge, test_df, not_in_edge_lis)\n",
    "    x_phone2idx = _add_phone2ix(x_phone2idx, not_in_edge_lis)\n",
    "    print(len(x), len(edge), len(x_phone2idx))\n",
    "    fake_labels, train_mask, valid_mask, test_mask = _get_final_label(test_df, x_phone2idx)\n",
    "    return x, edge, x_phone2idx, fake_labels, train_mask, valid_mask, test_mask\n",
    "    \n",
    "\n",
    "def read_cashbus_data(root):\n",
    "    \n",
    "    ##################### finish add test infos into the graph ############################\n",
    "    with open(root+'/feat.json') as json_file:\n",
    "        x = json.load(json_file)\n",
    "    with open(root+'/x_phone2idx.json') as json_file:\n",
    "        x_phone2idx = json.load(json_file)\n",
    "    with open(root+'/edge.json') as json_file:\n",
    "        edge = json.load(json_file)\n",
    "    labels = pd.read_csv(root+'/four_days_label.csv')\n",
    "    x, edge, x_phone2idx, y, train_mask, valid_mask, test_mask = get_all_info(x, edge, x_phone2idx, labels)  \n",
    "    \n",
    "    ##################### finish add test infos into the graph ############################\n",
    "    \n",
    "    feat_mat = []\n",
    "    for k,v in tqdm(x.items()):\n",
    "        feat_mat.append(v)\n",
    "    x = torch.tensor(feat_mat, dtype =torch.float)    \n",
    "    y = torch.tensor(y, dtype=torch.int64).squeeze()\n",
    "    edge = np.array(edge).T\n",
    "    row1 = [x_phone2idx[str(i)] for i in edge[0]]\n",
    "    row2 = [x_phone2idx[str(i)] for i in edge[1]]\n",
    "    new_edges = torch.tensor(np.stack([row1, row2]))\n",
    "    \n",
    "    ##################### finish all ############################\n",
    "    data = Data(x=x, edge_index=new_edges, y=y)\n",
    "    data.train_mask = torch.tensor(train_mask, dtype=torch.uint8)\n",
    "    data.val_mask = torch.tensor(valid_mask, dtype=torch.uint8)\n",
    "    data.test_mask = torch.tensor(test_mask, dtype=torch.uint8)\n",
    "    return data\n",
    "\n",
    "class CashBus(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(CashBus, self).__init__(root, transform, pre_transform)\n",
    "        print('processed_path:{}'.format(self.processed_paths))\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['feat.json', 'x_phone2idx.json', 'edge.json', 'four_days_label.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data11.pt'\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        print('go pl, raw_dir:{}'.format(self.raw_dir))\n",
    "        data = read_cashbus_data(self.raw_dir)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.name)\n",
    "\n",
    "st = time.time()\n",
    "root = '/home/qibo/all_project/Graph反欺诈/PYG/'\n",
    "dataset = CashBus(root)\n",
    "data = dataset[0]\n",
    "print(time.time() - st)\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16, cached=True)\n",
    "        self.conv1_1 = GCNConv(16, 16, cached=True)\n",
    "        self.conv1_2 = GCNConv(16, 16, cached=True)\n",
    "        self.conv2 = GCNConv(16, int(dataset.num_classes), cached=True)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.conv1_1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.conv1_2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        print(data.y[mask].sum().item() / len(data.y[mask]))\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 20):\n",
    "    print('epoch:{}'.format(epoch))\n",
    "    st = time.time()\n",
    "    train()\n",
    "    print('use {} seconds'.format(time.time() - st))\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_acc, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def to_temp_file(save_path, st, et, save=False):\n",
    "    test_day_file = '/data-0/tigergraph/30_W_samples/ctx_joined_table_phone.csv'\n",
    "    zz = pd.read_csv(test_day_file)\n",
    "    root = '/data-0/tigergraph/30_W_samples/label.csv'\n",
    "    temp = pd.read_csv(root)\n",
    "    final = temp.merge(zz, on='userid')\n",
    "    final = final[(final.created<=et)&(final.created>=st)]\n",
    "    if save:\n",
    "        final[['userid','username']].drop_duplicates().to_csv(save_path,header=False, index=False)\n",
    "    return final\n",
    "\n",
    "zz = to_temp_file('/data-0/tigergraph/30_W_samples/temp_phones_file.csv','2019-01-28','2019-03-28', True)\n",
    "res = ctx_get_neighs2('/data-0/tigergraph/30_W_samples/temp_phones_file.csv', st='2019-01-28',et='2019-03-28')\n",
    "\n",
    "\n",
    "####################### save edge version1 ###############################\n",
    "edge_df = pd.DataFrame.from_dict(res['results'][0]['@@node_neighs_edge'])\n",
    "edge_df2 = edge_df[['from_id', 'to_id']]\n",
    "edge_df2 = edge_df2.sort_values('from_id')\n",
    "with open('/data-0/tigergraph/30W_2/edge.json', 'w') as fp:\n",
    "    json.dump(edge_df2.values.tolist(), fp)\n",
    "    \n",
    "\n",
    "####################### save id2phone_dict  ###############################\n",
    "id2phone_dict = dict(enumerate(edge_df2.from_id.unique()))\n",
    "with open('/data-0/tigergraph/30W_2/id2phone_dic.json', 'w') as fp:\n",
    "    json.dump(id2phone_dict, fp)\n",
    "\n",
    "\n",
    "####################### open feat & save new feat  ###############################\n",
    "root = '/data-0/tigergraph/30_W_samples/feat.json'\n",
    "with open(root,'r') as f:\n",
    "    feat_dict = json.load(f)\n",
    "with open('/data-0/tigergraph/30W_2/id2phone_dic.json', 'r') as fp:\n",
    "    id2phone = json.load(fp)\n",
    "dic = {}\n",
    "error = []\n",
    "for idx, phone in tqdm(id2phone.items()):\n",
    "    if phone in feat_dict:\n",
    "        dic[phone] = feat_dict[phone]\n",
    "    else:\n",
    "        error.append(phone)\n",
    "with open('/data-0/tigergraph/30W_2/feat.json', 'w') as fp:\n",
    "    json.dump(dic, fp)\n",
    "    \n",
    "    \n",
    "####################### save edge version2(since error above) ###############################\n",
    "with open('/data-0/tigergraph/30W_2/edge.json', 'r') as fp:\n",
    "    edge = json.load(fp)\n",
    "    \n",
    "def remove_all_occ(lis, val):\n",
    "    return list(filter(lambda a: a[0] != val, lis))\n",
    "\n",
    "print(len(set([i[0] for i in edge])))\n",
    "\n",
    "edge = remove_all_occ(edge,'18863378590')\n",
    "edge = remove_all_occ(edge,'15708095469')\n",
    "\n",
    "print(len(set([i[0] for i in edge])))\n",
    "with open('/data-0/tigergraph/30W_2/edge.json', 'w') as fp:\n",
    "    json.dump(edge, fp)\n",
    "####################### save phone2id.json (since error above) ###############################\n",
    "\n",
    "root = '/data-0/tigergraph/30W_2/feat.json'\n",
    "with open(root,'r') as f:\n",
    "    feat_dict = json.load(f)\n",
    "\n",
    "phone2idx = {}\n",
    "for i,phone in enumerate(feat_dict.keys()):\n",
    "    phone2idx[phone] = i\n",
    "\n",
    "with open('/data-0/tigergraph/30W_2/phone2idx.json', 'w') as fp:\n",
    "    json.dump(phone2idx, fp)\n",
    "\n",
    "####################### save new_label_dic   ###############################\n",
    "groups = zz.groupby('loanid')\n",
    "label_dic = {}\n",
    "for i, g in tqdm(groups):\n",
    "    assert len(g) == 3\n",
    "    phone = g.username.tolist()[0]\n",
    "    label = g.default_20.tolist()[-2]\n",
    "    if phone in label_dic:\n",
    "        print('phone:{} has multi occurence'.format(phone))\n",
    "    label_dic[phone] = label\n",
    "\n",
    "with open('/data-0/tigergraph/30W_2/phone2idx.json', 'r') as fp:\n",
    "    phone2idx = json.load(fp)\n",
    "new_label_dic = {}\n",
    "for i in phone2idx:\n",
    "    new_label_dic[i] = label_dic[i]\n",
    "\n",
    "with open('/data-0/tigergraph/30W_2/label.json', 'w') as fp:\n",
    "     json.dump(new_label_dic, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
