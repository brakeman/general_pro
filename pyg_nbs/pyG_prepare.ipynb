{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import logging\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SplineConv,GCNConv\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "GRAPHSQL_URL = \"http://192.168.20.241:9000/query/OneMonthNet/\"\n",
    "\n",
    "############################ util functions ############################\n",
    "def query_graphsql_ori(query_name, para_string):\n",
    "    remaining_url = \"{}?{}\".format(query_name, para_string)\n",
    "    url=urljoin(GRAPHSQL_URL, remaining_url)\n",
    "    print('query:{}\\nparas:{}\\nrequest for url: {}'.format(query_name, para_string.split('&'), url))\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "    result = requests.get(url)\n",
    "    result_json = json.loads(result.text)\n",
    "    return result_json\n",
    "\n",
    "\n",
    "def query_graphsql(query_name, para_string):\n",
    "    remaining_url = \"{}?{}\".format(query_name, para_string)\n",
    "    url=urljoin(GRAPHSQL_URL, remaining_url)\n",
    "    st = time.time()\n",
    "    print('query:{}\\nparas:{}\\nrequest for url: {}'.format(query_name, para_string.split('&'), url))\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "    try:\n",
    "        result = requests.get(url)\n",
    "        result_json = json.loads(result.text)\n",
    "        if result_json['error']:\n",
    "            logging.error(result_json['message'])\n",
    "            print('run query failed')\n",
    "            return None\n",
    "        print('run query finish, use {} seconds\\n\\n'.format(time.time() - st))\n",
    "        return result_json\n",
    "    except Exception as e:\n",
    "        print('failed')\n",
    "        \n",
    "        \n",
    "def _update_max_min_date(node_name):\n",
    "    '''\n",
    "    get the max & min date for the given node type based on its history update dates;\n",
    "    para: node_name: 'Device'; check all the node_name type in gsql graph schema: OneMonthNet;\n",
    "    '''\n",
    "    query_name = 'update_max_min_date'\n",
    "    paras = 'node={}'.format(node_name)\n",
    "    return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "def _node_cutoff_filter(start_time, end_time, node_type):\n",
    "    '''\n",
    "    note that call _update_max_min_date before run this query;\n",
    "    select those nodes with given node_type exist between start_time and end_time;\n",
    "    paras:start_time:'2019-06-01 18:42:22'\n",
    "    paras:end_time:'2019-07-01 18:42:22'\n",
    "    paras:node_type:'User'; check all the node_name type in gsql graph schema: OneMonthNet;\n",
    "    '''\n",
    "    query_name = 'node_cutoff_filter'\n",
    "    paras = 'start_t={}&end_t={}&node={}'.format(start_time, end_time, node_type) \n",
    "    return query_graphsql(query_name, paras)\n",
    "    \n",
    "    \n",
    "########################################################################\n",
    "\n",
    "# def label_get(start_date):\n",
    "#     'old version and has been deprecated now, pls use label_get2 query;'\n",
    "#     query_name = 'label_get'\n",
    "#     paras = 'start_date={}'.format(start_date) \n",
    "#     return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "def label_get2(start_date, end_date, loanstyle):\n",
    "    '''\n",
    "    get loans with the given loanstyle whose fundtime between start_date & end_date;\n",
    "    paras: start_date: '2019-06-01 00:00:00'\n",
    "    paras: end_date: '2019-07-01 00:00:00'\n",
    "    paras: loanstyle: '绿卡30天1期'\n",
    "    '''\n",
    "    query_name = 'label_get2'\n",
    "    paras = 'start_date={}&end_date={}&loan_type={}'.format(start_date, end_date, loanstyle) \n",
    "    return query_graphsql(query_name, paras)['results'][0]['loanlabelSHOW']\n",
    "\n",
    "\n",
    "#1\n",
    "def reset(node):\n",
    "    '''\n",
    "    '''\n",
    "    query_name = 'reset'\n",
    "    paras = 'node={}'.format(node) \n",
    "    return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "#2\n",
    "def pageRank_train(start_t, end_t, node, maxChange, maxIter, damping, query_name):\n",
    "    '''\n",
    "    '''\n",
    "    paras = 'start_t={}&end_t={}&node={}&maxChange={}&maxIter={}&damping={}'.format(start_t, end_t, node, maxChange,maxIter,damping) \n",
    "    return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "#3\n",
    "def pageRank_appr_files(file_abs_path, query_name):\n",
    "    '''\n",
    "    draft 1 for pageRank in cashbus graph; \n",
    "    Only works for user-device relationship(single edge type & double vertex type);\n",
    "    Only output nodes(type: user or device) with pg_score !=0;\n",
    "    '''\n",
    "    paras = 'file_path={}'.format(file_abs_path) \n",
    "    return query_graphsql(query_name, paras) \n",
    "\n",
    "\n",
    "def pyG_prepare_label(st, et, node):\n",
    "    query_name = 'get_default_now'\n",
    "    paras = 'start_t={}&end_t={}&node={}'.format(st, et, node)\n",
    "    return query_graphsql(query_name, paras) \n",
    "\n",
    "    \n",
    "def pyG_prepare_edge(st, et, node):\n",
    "    query_name = 'pyG_pre'\n",
    "    paras = 'start_t={}&end_t={}&node={}'.format(st,et,node)\n",
    "    return query_graphsql(query_name, paras) \n",
    "\n",
    "\n",
    "def pyG_prepare_feat(st, et, node):\n",
    "    query_name = 'pyG_pre_ori'\n",
    "    paras = 'start_t={}&end_t={}&node={}'.format(st,et,node)\n",
    "    return query_graphsql(query_name, paras) \n",
    "\n",
    "\n",
    "#2\n",
    "def connected_comp_train(start_t, end_t, node):\n",
    "    '''\n",
    "    '''\n",
    "    query_name='conn_comp'\n",
    "    paras = 'start_t={}&end_t={}&node={}'.format(start_t, end_t, node) \n",
    "    return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "#3\n",
    "def connected_comp_appr_files(file_abs_path):\n",
    "    '''\n",
    "    draft 1 for pageRank in cashbus graph; \n",
    "    Only works for user-device relationship(single edge type & double vertex type);\n",
    "    Only output nodes(type: user or device) with pg_score != 0;\n",
    "    '''\n",
    "    paras = 'file_path={}'.format(file_abs_path)\n",
    "    query_name = 'conn_comp_check'\n",
    "    return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "def train(st, et, node_type, reset_bool):\n",
    "    \"\"\"\n",
    "    paras: st: start time of a period, only node within the restriction will be trained and given a pgscore;\n",
    "    paras: et: end time of the period;\n",
    "    paras: node_type: determine which nodetype will be trained;\n",
    "    \"\"\"\n",
    "    assert node_type in [\"Device\", \"PhoneNumber\"]\n",
    "    if reset_bool:\n",
    "        reset(node_type)\n",
    "    pageRank_train(st, \n",
    "                   et, \n",
    "                   node_type, \n",
    "                   10, 3, 0.6,\n",
    "                   \"pageRank_train_{}\".format(node_type.lower()))\n",
    "    return None\n",
    "\n",
    "\n",
    "def test_with_local_files(nodetype, file_path):\n",
    "    \"\"\"\n",
    "    paras: node_type: determine which nodetype will be test;\n",
    "    paras: file_path: only provide a path to save test nodes id;\n",
    "    \"\"\"\n",
    "    test_res2 = pageRank_appr_files(file_path, \n",
    "                                    \"pageRank_appr_files_{}\".format(nodetype.lower()))\n",
    "    user_pg_res = [i['attributes'] for i in test_res2['results'][0]['test_set']]\n",
    "    user_pg_df2 = pd.DataFrame.from_dict(user_pg_res)\n",
    "    user_pg_df2.rename(columns={'prim_id': 'username'}, inplace=True)\n",
    "    return test_res2, user_pg_df2\n",
    "\n",
    "\n",
    "def check_phone(phone):\n",
    "    paras = 'm1={}'.format(phone)\n",
    "    query_name = 'check_phone'\n",
    "    return query_graphsql(query_name, paras)\n",
    "\n",
    "\n",
    "def get_edge(res):\n",
    "    edge = []\n",
    "    for dic in res['results'][0]['vv']:\n",
    "        b = dic['v_id']\n",
    "        a = dic['attributes']['vv.@node_all_neighs']\n",
    "        bb = len(a)*[b]\n",
    "        temp_edge = list(zip(a, bb))\n",
    "        edge.extend(temp_edge)\n",
    "    return edge\n",
    "\n",
    "\n",
    "def get_feat(res_ori):\n",
    "    feat_DIC = {}\n",
    "    for dic in res_ori['results'][0]['vv']:\n",
    "        feat_dic = dic['attributes']['vv.@node_date_calls']\n",
    "        feat = []\n",
    "        v_id = dic['v_id']\n",
    "        for date in ['20190601', '20190602', '20190603', '20190604']:\n",
    "            if date not in feat_dic:\n",
    "                feat.append(0)\n",
    "            else:\n",
    "                feat.append(len(feat_dic[date]))\n",
    "        feat_DIC[v_id] = feat\n",
    "    return feat_DIC\n",
    "\n",
    "\n",
    "def get_label(res_label):\n",
    "    DIC = {}\n",
    "    for dic in res_label['results'][0]['vv']:\n",
    "        label = dic['attributes']['vv.@node_all_labels']\n",
    "        DIC[dic['v_id']]=label\n",
    "    return DIC\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare & save to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label1 = pd.read_csv('/data-0/gsm/qb_one_month/loan_label_1_1901_1907.csv')\n",
    "label2 = pd.read_csv('/data-0/gsm/qb_one_month/loan_label_2_new_1901_1907.csv')\n",
    "label2.columns = list(label1.columns)+['nouse']\n",
    "\n",
    "# xiaomajie 给的表；\n",
    "loan_detail_08_09_df = pd.read_csv('/data-0/qibo/Gdata/oneMonth/qb_temp_loan_detail_08_09.csv')\n",
    "loan_detail_08_09_df.username = loan_detail_08_09_df.username.astype(str)\n",
    "\n",
    "def test_loan_info(loanstyle, st, et):\n",
    "    # 给定两个时间范围，给出所在时间内的 funded loan 信息；\n",
    "    temp1 = label1[(label1.funddate<et)&(label1.funddate>=st)]\n",
    "    temp2 = label2[(label2.funddate<et)&(label2.funddate>=st)]\n",
    "    final = temp1.append(temp2)\n",
    "    if loanstyle == 'all':\n",
    "        return final\n",
    "    return final[final.loanstyle == loanstyle]\n",
    "\n",
    "def source_table(loanstyle, st, et):\n",
    "    #xiaomajie biao;\n",
    "    test_funded_loans = loan_detail_08_09_df[(loan_detail_08_09_df.fundtime>=st)&(loan_detail_08_09_df.fundtime<et)]\n",
    "    #label 表;\n",
    "    test_label = test_loan_info(loanstyle, st, et)\n",
    "    #merge;\n",
    "    table3 = pd.merge(test_funded_loans, test_label, on='loanid')\n",
    "    return table3\n",
    "\n",
    "test_st='2019-06-01 00:00:00'\n",
    "test_et='2019-06-05 00:00:00'\n",
    "\n",
    "source_df = source_table('绿卡30天1期', test_st, test_et)\n",
    "four_days_label = source_df[['username', 'fundtime', 'default_now']]\n",
    "\n",
    "root ='/home/qibo/all_project/Graph反欺诈/PYG/raw'\n",
    "four_days_label.to_csv(root+'/four_days_label.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:pyG_pre\n",
      "paras:['start_t=20190601', 'end_t=20190604', 'node=PhoneNumber']\n",
      "request for url: http://192.168.20.241:9000/query/OneMonthNet/pyG_pre?start_t=20190601&end_t=20190604&node=PhoneNumber\n",
      "---------------------------------------------------------------------------------------------------\n",
      "run query finish, use 94.4853093624115 seconds\n",
      "\n",
      "\n",
      "query:pyG_pre_ori\n",
      "paras:['start_t=20190601', 'end_t=20190604', 'node=PhoneNumber']\n",
      "request for url: http://192.168.20.241:9000/query/OneMonthNet/pyG_pre_ori?start_t=20190601&end_t=20190604&node=PhoneNumber\n",
      "---------------------------------------------------------------------------------------------------\n",
      "run query finish, use 115.0154185295105 seconds\n",
      "\n",
      "\n",
      "query:get_default_now\n",
      "paras:['start_t=20190601', 'end_t=20190604', 'node=PhoneNumber']\n",
      "request for url: http://192.168.20.241:9000/query/OneMonthNet/get_default_now?start_t=20190601&end_t=20190604&node=PhoneNumber\n",
      "---------------------------------------------------------------------------------------------------\n",
      "run query finish, use 58.50489544868469 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_st='20190601'\n",
    "test_et='20190604'\n",
    "\n",
    "res_edge = pyG_prepare_edge(test_st, test_et, \"PhoneNumber\")\n",
    "res_feat = pyG_prepare_feat(test_st, test_et, \"PhoneNumber\")\n",
    "# res_label = pyG_prepare_label(test_st, test_et, \"PhoneNumber\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_DIC = get_feat(res_feat)\n",
    "edge = get_edge(res_edge)\n",
    "\n",
    "source = set(feat_DIC.keys())\n",
    "a1 = np.array(edge).transpose()[0]\n",
    "a2 = np.array(edge).transpose()[1]\n",
    "edge_set = set(a1).union(set(a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. load and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(root+'raw/feat.json') as json_file:\n",
    "    x = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>07962104310</th>\n",
       "      <th>077195232611</th>\n",
       "      <th>07962133824</th>\n",
       "      <th>077195202442</th>\n",
       "      <th>089895147706</th>\n",
       "      <th>07902106988</th>\n",
       "      <th>077195241135</th>\n",
       "      <th>07992089059</th>\n",
       "      <th>087168284381</th>\n",
       "      <th>09513842096</th>\n",
       "      <th>...</th>\n",
       "      <th>15260014761</th>\n",
       "      <th>15260138012</th>\n",
       "      <th>13656077398</th>\n",
       "      <th>15265355431</th>\n",
       "      <th>15261115158</th>\n",
       "      <th>15107020440</th>\n",
       "      <th>15116227437</th>\n",
       "      <th>13792499758</th>\n",
       "      <th>13780678904</th>\n",
       "      <th>13789218135</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 5938366 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   07962104310  077195232611  07962133824  077195202442  089895147706  \\\n",
       "0            7             0            4             1             1   \n",
       "1            4             0            9             0             2   \n",
       "2            4             2            1             1             1   \n",
       "3            4             0            7             1             1   \n",
       "\n",
       "   07902106988  077195241135  07992089059  087168284381  09513842096  ...  \\\n",
       "0            0             0            1            14           13  ...   \n",
       "1            1             0            0            12           10  ...   \n",
       "2            1             0            0            20           10  ...   \n",
       "3            1             1            3            15            9  ...   \n",
       "\n",
       "   15260014761  15260138012  13656077398  15265355431  15261115158  \\\n",
       "0            1            1            1            1            1   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            0            0            0            0            0   \n",
       "\n",
       "   15107020440  15116227437  13792499758  13780678904  13789218135  \n",
       "0            1            1            1            1            1  \n",
       "1            0            0            0            0            0  \n",
       "2            0            0            0            0            0  \n",
       "3            0            0            0            0            0  \n",
       "\n",
       "[4 rows x 5938366 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root ='/home/qibo/all_project/Graph反欺诈/PYG/raw'\n",
    "\n",
    "# with open(root+'/feat.json') as json_file:\n",
    "#     x = json.load(json_file)\n",
    "    \n",
    "# with open(root+'/x_phone2idx.json') as json_file:\n",
    "#     x_phone2idx = json.load(json_file)\n",
    "\n",
    "# with open(root+'/edge.json') as json_file:\n",
    "#     edge = json.load(json_file)\n",
    "\n",
    "# root ='/home/qibo/all_project/Graph反欺诈/PYG/raw/'\n",
    "# labels = pd.read_csv(root+'four_days_label.csv')\n",
    "\n",
    "\n",
    "def __get_mask_idx(labels_df, train_valid_split=0.8):\n",
    "    train_idx = labels_df[labels_df.fundtime<'2019-06-03 00:00:00'].username.tolist()\n",
    "    test_idx = labels_df[labels_df.fundtime>='2019-06-04 00:00:00'].username.tolist()\n",
    "    split_idx = int(len(train_idx)*train_valid_split)\n",
    "    train = train_idx[:split_idx]\n",
    "    valid = train_idx[split_idx:]\n",
    "    print(len(train), len(valid), len(test_idx))\n",
    "    return train, valid, test_idx\n",
    "\n",
    "def _get_final_label(labels, new_x_phone2idx):\n",
    "    \n",
    "    label_dict = labels[['username', 'default_now']].set_index('username').to_dict()['default_now']\n",
    "    train_phone, valid_phone, test_phone = __get_mask_idx(labels)\n",
    "    fake_labels = np.zeros(len(new_x_phone2idx))-1\n",
    "\n",
    "    for phone in label_dict:\n",
    "        idx = new_x_phone2idx[str(phone)]\n",
    "        fake_labels[idx] = label_dict[int(phone)]   \n",
    "\n",
    "    train_mask = [new_x_phone2idx[str(phone)] for phone in train_phone]\n",
    "    fake_train_labels = np.zeros(len(new_x_phone2idx))\n",
    "    fake_train_labels[train_mask] = 1\n",
    "\n",
    "    valid_mask = [new_x_phone2idx[str(phone)] for phone in valid_phone]\n",
    "    fake_valid_labels = np.zeros(len(new_x_phone2idx))\n",
    "    fake_valid_labels[valid_mask] = 1\n",
    "    \n",
    "    test_mask = [new_x_phone2idx[str(phone)] for phone in test_phone]\n",
    "    fake_test_labels = np.zeros(len(new_x_phone2idx))\n",
    "    fake_test_labels[test_mask] = 1\n",
    "    \n",
    "    return fake_labels, fake_train_labels, fake_valid_labels, fake_test_labels\n",
    "\n",
    "\n",
    "def _add_new_feat(x, test_df):\n",
    "    not_in_edge_lis = []\n",
    "    for i in test_df.username:\n",
    "        if str(i) not in x:\n",
    "            x[str(i)] = [0, 0, 0, 0]\n",
    "            not_in_edge_lis.append(str(i))\n",
    "    return x, not_in_edge_lis\n",
    "\n",
    "\n",
    "def _add_new_edge(edge, test_df, not_in_edge_lis):\n",
    "    for i in test_df.username:\n",
    "        if str(i) in not_in_edge_lis:\n",
    "            edge.append([str(i), str(i)])\n",
    "    return edge\n",
    "    \n",
    "def _add_phone2ix(x_phone2idx, not_in_edge_lis):\n",
    "    old_len = len(x_phone2idx)\n",
    "    i=0\n",
    "    for new_phone in not_in_edge_lis:\n",
    "        x_phone2idx[new_phone] = old_len+i\n",
    "        i+=1\n",
    "    return x_phone2idx\n",
    "\n",
    "def get_all_info(x, edge, x_phone2idx, test_df):\n",
    "    print(len(x), len(edge), len(x_phone2idx))\n",
    "    print('--------------------------')\n",
    "    x, not_in_edge_lis = _add_new_feat(x, test_df)\n",
    "    edge = _add_new_edge(edge, test_df, not_in_edge_lis)\n",
    "    x_phone2idx = _add_phone2ix(x_phone2idx, not_in_edge_lis)\n",
    "    print(len(x), len(edge), len(x_phone2idx))\n",
    "    fake_labels, train_mask, valid_mask, test_mask = _get_final_label(test_df, x_phone2idx)\n",
    "    return x, edge, x_phone2idx, fake_labels, train_mask, valid_mask, test_mask\n",
    "    \n",
    "\n",
    "def read_cashbus_data(root):\n",
    "    \n",
    "    ##################### finish add test infos into the graph ############################\n",
    "    with open(root+'/feat.json') as json_file:\n",
    "        x = json.load(json_file)\n",
    "    with open(root+'/x_phone2idx.json') as json_file:\n",
    "        x_phone2idx = json.load(json_file)\n",
    "    with open(root+'/edge.json') as json_file:\n",
    "        edge = json.load(json_file)\n",
    "    labels = pd.read_csv(root+'/four_days_label.csv')\n",
    "    x, edge, x_phone2idx, y, train_mask, valid_mask, test_mask = get_all_info(x, edge, x_phone2idx, labels)  \n",
    "    \n",
    "    ##################### finish add test infos into the graph ############################\n",
    "    \n",
    "    feat_mat = []\n",
    "    for k,v in tqdm(x.items()):\n",
    "        feat_mat.append(v)\n",
    "    x = torch.tensor(feat_mat, dtype =torch.float)    \n",
    "    y = torch.tensor(y).squeeze()\n",
    "    edge = np.array(edge).T\n",
    "    row1 = [x_phone2idx[str(i)] for i in edge[0]]\n",
    "    row2 = [x_phone2idx[str(i)] for i in edge[1]]\n",
    "    new_edges = torch.tensor(np.stack([row1, row2]))\n",
    "    \n",
    "    ##################### finish all ############################\n",
    "    data = Data(x=x, edge_index=new_edges, y=y)\n",
    "    data.train_mask = torch.tensor(train_mask, dtype=torch.uint8)\n",
    "    data.val_mask = torch.tensor(valid_mask, dtype=torch.uint8)\n",
    "    data.test_mask = torch.tensor(test_mask, dtype=torch.uint8)\n",
    "    return data\n",
    "\n",
    "class CashBus(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(CashBus, self).__init__(root, transform, pre_transform)\n",
    "        print('processed_path:{}'.format(self.processed_paths))\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['feat.json', 'x_phone2idx.json', 'edge.json', 'four_days_label.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data8.pt'\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        print('go pl, raw_dir:{}'.format(self.raw_dir))\n",
    "        data = read_cashbus_data(self.raw_dir)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_path:['/home/qibo/all_project/Graph反欺诈/PYG/processed/data8.pt']\n"
     ]
    }
   ],
   "source": [
    "root = '/home/qibo/all_project/Graph反欺诈/PYG/'\n",
    "dataset = CashBus(root)\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC0DUMLKlzc/kR/oJcgUWDVhuAu0KIJC1LmZOV6coAZhABu7R7G71my0LdDdLOPpwojyybGI7VyaUw4pdhjaleOYs0p4m6tHAkIb862BfJ8d8MStknZQPGr2i0bOsu6pDyxLB4KlxhID1aHJz1UhlJ5If2cetDKDe+CSf43zAPAepK86yHTtkZIAlMMNkM1oTv9Ly511vO0aoQaIW7oWKdyXHmIIuO0IleQOKhwPYjSLDQGozfyLVI+FCgjK9EOTI6K8NYsqVVuY3CTGNr/EzCeof/Ep3uv7a8RMQqgnn7CzELfie1YX/0pwmsPimMwgUwR4vrMrCGoFcptxAR/h+4Qe/RkqDr1W5dfH+Qx5nqDD0MfHZEL6M6WlKFfQp893tSzU1THNj0eDNqRVv97Jo/iOuEYO9az2jnWq59NoelNb32eIFaxpsHDVY5IOGW96D8DF+tj6A3ZzprpCxUAlmHoxoVYwVRasW6zq7BC7zMzhs5Kw6pfTIfWS3HtOeHkT+wd0a842N7MugKpViu72fwP6JcyeXVztJTzEFv6N+sivvoRT1QbGUUz08oxEZ+SmFbrKYbKvdTxInZB+zDl3+nftnlP3XzAooGtug8CpcEkDUiYJcrt1NJej1vw2NtVAJjCJUoVy0xURqdb9EbOGjHMNEUsHJ0jykDmdNw7H6x+HQ== qibo_2018@outlook.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDO53rheAywg+H/m5Ob+/5QKgiAJ4DQMADqCgInoL3cmJGLs8bweCDEugBdHz/eM8itwL9yhilnM0oTEHzn/WiA/Jw3ZvdANcu0u1q4JBwQx4/mfJ9jpwj0H9s5EPWmkfGt5mfWYa41zBqTvC/muIi1waTBzBTH95c/RS0hTMKt2OUOieVbBBdAqGDKQSTd3BHoxR74/cyTZ3H+iOBtsrxzOM4WC7iMVEgImRs8ffsqhxl4k0KXCJXxyOUxqgZDK5ZoXYmgyH4+pQEiLv9pNP97Aqo5hTGFIvglypMeBLp/cg699T5naVhzCZ0B/wjVt3Qy3Xxvtt/X4qs9nUhOqlkNMfWAtzKhLtfiOIOlINnq4W9ftCaA63HCJxFbpVPqyb8lCZM9xXQcnWOds2rIabmBuU7OOwLy8aO9U+21AJgWzKCiHhRPmsiXmJTE+nBX+57ChoxYJBGZhMjIVk5NypfOA3vPWraSWt+/q8MXn4Wkg241PyoLaUVAlP647n/xJu16Ukr2QVD5Wi2Ni1Q7W0zuUX8Z1rahwqK4kL5WyrI3l2go8j40+cHaAr6hPImzeGAo02lZvNUUR00YqrnThgMa7qwU8KPcmvn32ke+NG7oF+fCOf7UVRse3TPjbeNJtoeHQ/K8sWL+8HqdMyPLODnP4HZ/xJsta7gh/npP9L5Q9w== qibo_2018@outlook.com\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
