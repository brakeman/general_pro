{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path as osp\n",
    "import torch\n",
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.nn import GATConv\n",
    "from sklearn.metrics import f1_score\n",
    "from networkx.readwrite import json_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeniePathLazy(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GeniePathLazy, self).__init__()\n",
    "        self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "        self.breadths = torch.nn.ModuleList(\n",
    "            [Breadth(dim, dim) for i in range(layer_num)])\n",
    "        self.depths = torch.nn.ModuleList(\n",
    "            [Depth(dim * 2, lstm_hidden) for i in range(layer_num)])\n",
    "        self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.lin1(x)\n",
    "        h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "        h_tmps = []\n",
    "        for i, l in enumerate(self.breadths):\n",
    "            h_tmps.append(self.breadths[i](x, edge_index))\n",
    "        x = x[None, :]\n",
    "        for i, l in enumerate(self.depths):\n",
    "            in_cat = torch.cat((h_tmps[i][None, :], x), -1)\n",
    "            x, (h, c) = self.depths[i](in_cat, h, c)\n",
    "        x = self.lin2(x[0])\n",
    "        return x\n",
    "\n",
    "class Breadth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Breadth, self).__init__()\n",
    "        self.gatconv = GATConv(in_dim, out_dim, heads=1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.tanh(self.gatconv(x, edge_index))\n",
    "        return x\n",
    "\n",
    "class Depth(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden):\n",
    "        super(Depth, self).__init__()\n",
    "        self.lstm = torch.nn.LSTM(in_dim, hidden, 1, bias=False)\n",
    "    def forward(self, x, h, c):\n",
    "        x, (h, c) = self.lstm(x, (h, c))\n",
    "        return x, (h, c)\n",
    "\n",
    "    \n",
    "# class GeniePathLayer(torch.nn.Module):\n",
    "#     def __init__(self, in_dim):\n",
    "#         super(GeniePathLayer, self).__init__()\n",
    "#         self.breadth_func = Breadth(in_dim, dim)\n",
    "#         self.depth_func = Depth(dim, lstm_hidden)\n",
    "#     def forward(self, x, edge_index, h, c):\n",
    "#         x = self.breadth_func(x, edge_index)\n",
    "#         x = x[None, :]\n",
    "#         x, (h, c) = self.depth_func(x, h, c)\n",
    "#         x = x[0]\n",
    "#         return x, (h, c)\n",
    "\n",
    "    \n",
    "# class GeniePath(torch.nn.Module):\n",
    "#     def __init__(self, in_dim, out_dim):\n",
    "#         super(GeniePath, self).__init__()\n",
    "#         self.lin1 = torch.nn.Linear(in_dim, dim)\n",
    "#         self.gplayers = torch.nn.ModuleList(\n",
    "#             [GeniePathLayer(dim) for i in range(layer_num)])\n",
    "#         self.lin2 = torch.nn.Linear(dim, out_dim)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         x = self.lin1(x)\n",
    "#         h = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "#         c = torch.zeros(1, x.shape[0], lstm_hidden, device=x.device)\n",
    "#         for i, l in enumerate(self.gplayers):\n",
    "#             x, (h, c) = self.gplayers[i](x, edge_index, h, c)\n",
    "#         x = self.lin2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        num_graphs = data.num_graphs\n",
    "        data.batch = None\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_op(model(data.x, data.edge_index), data.y)\n",
    "        total_loss += loss.item() * num_graphs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    for data in loader:\n",
    "        ys.append(data.y)\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x.to(device), data.edge_index.to(device))\n",
    "        preds.append((out > 0).float().cpu())\n",
    "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\n",
    "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PPI(root='./')\n",
    "train_dataset = PPI('./', split='train')\n",
    "val_dataset = PPI('./', split='val')\n",
    "test_dataset = PPI('./', split='test')\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "dim = 256\n",
    "lstm_hidden = 256\n",
    "layer_num = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "# kwargs = {'GeniePath': GeniePath, 'GeniePathLazy': GeniePathLazy}\n",
    "model = GeniePathLazy(train_dataset.num_features,\n",
    "                           train_dataset.num_classes).to(device)\n",
    "loss_op = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3541159254439488\n",
      "10 0.7400444352825305\n",
      "20 0.8568920961529084\n",
      "30 0.8479616947831412\n",
      "40 0.8741310604144098\n",
      "50 0.9059154880146002\n",
      "60 0.8861367618972613\n",
      "70 0.8976634014932785\n",
      "80 0.9120209360671339\n",
      "90 0.915925093112699\n",
      "100 0.8900664634801938\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 101):\n",
    "    if epoch%10==0:\n",
    "        val_f1 = test(val_loader)\n",
    "        print(epoch, val_f1)\n",
    "    loss = train()\n",
    "#     test_f1 = test(test_loader)\n",
    "#     print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\n",
    "#         epoch, loss, val_f1, test_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 拆解两个数据集 ppi + cora, 通过双要素重构(边，点) dataset; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ppi.zip\n",
      "Extracting ./ppi.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# dataset = PPI(root='./')\n",
    "train_dataset = PPI('./', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch_geometric.datasets.ppi.PPI,\n",
       " torch_geometric.data.in_memory_dataset.InMemoryDataset,\n",
       " torch_geometric.data.dataset.Dataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__class__.__mro__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/qibo/all_project/Graph反欺诈/PYG/raw'\n",
    "feat_path = root + '/train_feats.npy'\n",
    "label_path = root + '/train_labels.npy'\n",
    "graph_path = root + '/train_graph.json'\n",
    "graph_id = root + '/train_graph_id.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.load(feat_path)\n",
    "labels = np.load(label_path)\n",
    "graph_ids = np.load(graph_id)\n",
    "with open(graph_path, 'r') as f:\n",
    "    graph = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process 逻辑；\n",
    "\n",
    "1. 尽管存在多个子图，不管此维度，拿到全量 node_feats(44906, 50), labels(44906), edges(1271274), 同时拿到各点子图id(44906);\n",
    "   \n",
    "2. 对每个子图， 将子图到 edge(34085), x(1767, 50), y(1767) 放入Data中 实例化； "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset base class for creating graph datasets.\n",
    "    See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
    "    create_dataset.html>`__ for the accompanying tutorial.\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        pre_filter (callable, optional): A function that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "            value, indicating whether the data object should be included in the\n",
    "            final dataset. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        r\"\"\"The name of the files to find in the :obj:`self.raw_dir` folder in\n",
    "        order to skip the download.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        r\"\"\"The name of the files to find in the :obj:`self.processed_dir`\n",
    "        folder in order to skip the processing.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def download(self):\n",
    "        r\"\"\"Downloads the dataset to the :obj:`self.raw_dir` folder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def process(self):\n",
    "        r\"\"\"Processes the dataset to the :obj:`self.processed_dir` folder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        r\"\"\"The number of examples in the dataset.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get(self, idx):\n",
    "        r\"\"\"Gets the data object at index :obj:`idx`.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None):\n",
    "        super(Dataset, self).__init__()\n",
    "\n",
    "        self.root = osp.expanduser(osp.normpath(root))\n",
    "        self.raw_dir = osp.join(self.root, 'raw')\n",
    "        self.processed_dir = osp.join(self.root, 'processed')\n",
    "        self.transform = transform\n",
    "        self.pre_transform = pre_transform\n",
    "        self.pre_filter = pre_filter\n",
    "        self._download()\n",
    "        self._process()\n",
    "\n",
    "    @property\n",
    "    def num_node_features(self):\n",
    "        r\"\"\"Returns the number of features per node in the dataset.\"\"\"\n",
    "        return self[0].num_node_features\n",
    "\n",
    "    @property\n",
    "    def num_features(self):\n",
    "        r\"\"\"Alias for :py:attr:`~num_node_features`.\"\"\"\n",
    "        return self.num_node_features\n",
    "\n",
    "    @property\n",
    "    def num_edge_features(self):\n",
    "        r\"\"\"Returns the number of features per edge in the dataset.\"\"\"\n",
    "        return self[0].num_edge_features\n",
    "\n",
    "    @property\n",
    "    def raw_paths(self):\n",
    "        r\"\"\"The filepaths to find in order to skip the download.\"\"\"\n",
    "        files = to_list(self.raw_file_names)\n",
    "        return [osp.join(self.raw_dir, f) for f in files]\n",
    "\n",
    "    @property\n",
    "    def processed_paths(self):\n",
    "        r\"\"\"The filepaths to find in the :obj:`self.processed_dir`\n",
    "        folder in order to skip the processing.\"\"\"\n",
    "        files = to_list(self.processed_file_names)\n",
    "        return [osp.join(self.processed_dir, f) for f in files]\n",
    "\n",
    "    def _download(self):\n",
    "        if files_exist(self.raw_paths):  # pragma: no cover\n",
    "            return\n",
    "        makedirs(self.raw_dir)\n",
    "        self.download()\n",
    "\n",
    "    def _process(self):\n",
    "        if files_exist(self.processed_paths):  # pragma: no cover\n",
    "            return\n",
    "        print('Processing...')\n",
    "        makedirs(self.processed_dir)\n",
    "        self.process()\n",
    "        print('Done!')\n",
    "\n",
    "    def __getitem__(self, idx):  # pragma: no cover\n",
    "        r\"\"\"Gets the data object at index :obj:`idx` and transforms it (in case\n",
    "        a :obj:`self.transform` is given).\"\"\"\n",
    "        data = self.get(idx)\n",
    "        data = data if self.transform is None else self.transform(data)\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):  # pragma: no cover\n",
    "        return '{}({})'.format(self.__class__.__name__, len(self))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryDataset(Dataset):\n",
    "    r\"\"\"Dataset base class for creating graph datasets which fit completely\n",
    "    into memory.\n",
    "    See `here <https://pytorch-geometric.readthedocs.io/en/latest/notes/\n",
    "    create_dataset.html#creating-in-memory-datasets>`__ for the accompanying\n",
    "    tutorial.\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        pre_filter (callable, optional): A function that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a boolean\n",
    "            value, indicating whether the data object should be included in the\n",
    "            final dataset. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        r\"\"\"The name of the files to find in the :obj:`self.raw_dir` folder in\n",
    "        order to skip the download.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        r\"\"\"The name of the files to find in the :obj:`self.processed_dir`\n",
    "        folder in order to skip the processing.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def download(self):\n",
    "        r\"\"\"Downloads the dataset to the :obj:`self.raw_dir` folder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def process(self):\n",
    "        r\"\"\"Processes the dataset to the :obj:`self.processed_dir` folder.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __init__(self, root, transform=None, pre_transform=None,\n",
    "                 pre_filter=None):\n",
    "        super(InMemoryDataset, self).__init__(root, transform, pre_transform,\n",
    "                                              pre_filter)\n",
    "        self.data, self.slices = None, None\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        r\"\"\"The number of classes in the dataset.\"\"\"\n",
    "        data = self.data\n",
    "        return data.y.max().item() + 1 if data.y.dim() == 1 else data.y.size(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.slices[list(self.slices.keys())[0]].size(0) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r\"\"\"Gets the data object at index :obj:`idx` and transforms it (in case\n",
    "        a :obj:`self.transform` is given).\n",
    "        Returns a data object, if :obj:`idx` is a scalar, and a new dataset in\n",
    "        case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a LongTensor\n",
    "        or a ByteTensor.\"\"\"\n",
    "        if isinstance(idx, int):\n",
    "            data = self.get(idx)\n",
    "            data = data if self.transform is None else self.transform(data)\n",
    "            return data\n",
    "        elif isinstance(idx, slice):\n",
    "            return self.__indexing__(range(*idx.indices(len(self))))\n",
    "        elif torch.is_tensor(idx) and idx.dtype == torch.long:\n",
    "            return self.__indexing__(idx)\n",
    "        elif torch.is_tensor(idx) and idx.dtype == torch.uint8:\n",
    "            return self.__indexing__(idx.nonzero())\n",
    "\n",
    "        raise IndexError(\n",
    "            'Only integers, slices (`:`) and long or byte tensors are valid '\n",
    "            'indices (got {}).'.format(type(idx).__name__))\n",
    "\n",
    "    def shuffle(self, return_perm=False):\n",
    "        r\"\"\"Randomly shuffles the examples in the dataset.\n",
    "        Args:\n",
    "            return_perm (bool, optional): If set to :obj:`True`, will\n",
    "                additionally return the random permutation used to shuffle the\n",
    "                dataset. (default: :obj:`False`)\n",
    "        \"\"\"\n",
    "        perm = torch.randperm(len(self))\n",
    "        dataset = self.__indexing__(perm)\n",
    "        return (dataset, perm) if return_perm is True else dataset\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = self.data.__class__()\n",
    "\n",
    "        if hasattr(self.data, '__num_nodes__'):\n",
    "            data.num_nodes = self.data.__num_nodes__[idx]\n",
    "\n",
    "        for key in self.data.keys:\n",
    "            item, slices = self.data[key], self.slices[key]\n",
    "            s = list(repeat(slice(None), item.dim()))\n",
    "            s[self.data.__cat_dim__(key, item)] = slice(\n",
    "                slices[idx], slices[idx + 1])\n",
    "            data[key] = item[s]\n",
    "        return data\n",
    "\n",
    "    def __indexing__(self, index):\n",
    "        copy = self.__class__.__new__(self.__class__)\n",
    "        copy.__dict__ = self.__dict__.copy()\n",
    "        copy.data, copy.slices = self.collate([self.get(i) for i in index])\n",
    "        return copy\n",
    "\n",
    "    def collate(self, data_list):\n",
    "        r\"\"\"Collates a python list of data objects to the internal storage\n",
    "        format of :class:`torch_geometric.data.InMemoryDataset`.\"\"\"\n",
    "        keys = data_list[0].keys\n",
    "        data = data_list[0].__class__()\n",
    "        for key in keys:\n",
    "            data[key] = []\n",
    "        slices = {key: [0] for key in keys}\n",
    "        for item, key in product(data_list, keys):\n",
    "            data[key].append(item[key])\n",
    "            if torch.is_tensor(item[key]):\n",
    "                s = slices[key][-1] + item[key].size(\n",
    "                    item.__cat_dim__(key, item[key]))\n",
    "            elif isinstance(item[key], int) or isinstance(item[key], float):\n",
    "                s = slices[key][-1] + 1\n",
    "            else:\n",
    "                raise ValueError('Unsupported attribute type')\n",
    "            slices[key].append(s)\n",
    "        if hasattr(data_list[0], '__num_nodes__'):\n",
    "            data.__num_nodes__ = []\n",
    "            for item in data_list:\n",
    "                data.__num_nodes__.append(item.num_nodes)\n",
    "        for key in keys:\n",
    "            if torch.is_tensor(data_list[0][key]):\n",
    "                data[key] = torch.cat(\n",
    "                    data[key], dim=data.__cat_dim__(key, data_list[0][key]))\n",
    "            else:\n",
    "                data[key] = torch.tensor(data[key])\n",
    "            slices[key] = torch.tensor(slices[key], dtype=torch.long)\n",
    "        return data, slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPI(InMemoryDataset):\n",
    "    url = 'https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/ppi.zip'\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 split='train',\n",
    "                 transform=None,\n",
    "                 pre_transform=None,\n",
    "                 pre_filter=None):\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        super(PPI, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        if split == 'train':\n",
    "            self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        elif split == 'val':\n",
    "            self.data, self.slices = torch.load(self.processed_paths[1])\n",
    "        elif split == 'test':\n",
    "            self.data, self.slices = torch.load(self.processed_paths[2])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        splits = ['train', 'valid', 'test']\n",
    "        files = ['feats.npy', 'graph_id.npy', 'graph.json', 'labels.npy']\n",
    "        return ['{}_{}'.format(s, f) for s, f in product(splits, files)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['train.pt', 'val.pt', 'test.pt']\n",
    "\n",
    "    def download(self):\n",
    "        path = download_url(self.url, self.root)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "\n",
    "    def process(self):\n",
    "        for s, split in enumerate(['train', 'valid', 'test']):\n",
    "            path = osp.join(self.raw_dir, '{}_graph.json').format(split)\n",
    "            with open(path, 'r') as f:\n",
    "                G = nx.DiGraph(json_graph.node_link_graph(json.load(f)))\n",
    "            x = np.load(osp.join(self.raw_dir, '{}_feats.npy').format(split))\n",
    "            x = torch.from_numpy(x).to(torch.float)\n",
    "            y = np.load(osp.join(self.raw_dir, '{}_labels.npy').format(split))\n",
    "            y = torch.from_numpy(y).to(torch.float)\n",
    "            data_list = []\n",
    "            path = osp.join(self.raw_dir, '{}_graph_id.npy').format(split)\n",
    "            idx = torch.from_numpy(np.load(path)).to(torch.long)\n",
    "            idx = idx - idx.min()\n",
    "            for i in range(idx.max().item() + 1):\n",
    "                mask = idx == i\n",
    "                G_s = G.subgraph(mask.nonzero().view(-1).tolist())\n",
    "                edge_index = torch.tensor(list(G_s.edges)).t().contiguous()\n",
    "                edge_index = edge_index - edge_index.min()\n",
    "                edge_index, _ = remove_self_loops(edge_index)\n",
    "                data = Data(edge_index=edge_index, x=x[mask], y=y[mask])\n",
    "                if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                    continue\n",
    "                if self.pre_transform is not None:\n",
    "                    data = self.pre_transform(data)\n",
    "                data_list.append(data)\n",
    "            torch.save(self.collate(data_list), self.processed_paths[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cora 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/qibo/all_project/Graph反欺诈/PYG/raw'\n",
    "feat_path = root + '/cora.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(feat_path) as f:\n",
    "    a = parse_npz(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([19793, 8710])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset\n",
    "class CoraFull(InMemoryDataset):\n",
    "    r\"\"\"The full Cora citation network dataset from the\n",
    "    `\"Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via\n",
    "    Ranking\" <https://arxiv.org/abs/1707.03815>`_ paper.\n",
    "    Nodes represent documents and edges represent citation links.\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://github.com/abojchevski/graph2gauss/raw/master/data/cora.npz'\n",
    "\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(CoraFull, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return 'cora.npz'\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        download_url(self.url, self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        data = read_npz(self.raw_paths[0])\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/abojchevski/graph2gauss/raw/master/data/cora.npz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from itertools import repeat\n",
    "from torch_geometric.datasets import CoraFull\n",
    "dataset = CoraFull(root='./')\n",
    "# data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "from torch_geometric.read import read_planetoid_data\n",
    "\n",
    "\n",
    "class Planetoid(InMemoryDataset):\n",
    "    r\"\"\"The citation network datasets \"Cora\", \"CiteSeer\" and \"PubMed\" from the\n",
    "    `\"Revisiting Semi-Supervised Learning with Graph Embeddings\"\n",
    "    <https://arxiv.org/abs/1603.08861>`_ paper.\n",
    "    Nodes represent documents and edges represent citation links.\n",
    "    Training, validation and test splits are given by binary masks.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory where the dataset should be saved.\n",
    "        name (string): The name of the dataset (:obj:`\"Cora\"`,\n",
    "            :obj:`\"CiteSeer\"`, :obj:`\"PubMed\"`).\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://github.com/kimiyoung/planetoid/raw/master/data'\n",
    "\n",
    "    def __init__(self, root, name, transform=None, pre_transform=None):\n",
    "        self.name = name\n",
    "        super(Planetoid, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        names = ['x', 'tx', 'allx', 'y', 'ty', 'ally', 'graph', 'test.index']\n",
    "        return ['ind.{}.{}'.format(self.name.lower(), name) for name in names]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self):\n",
    "        for name in self.raw_file_names:\n",
    "            download_url('{}/{}'.format(self.url, name), self.raw_dir)\n",
    "\n",
    "    def process(self):\n",
    "        data = read_planetoid_data(self.raw_dir, self.name)\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cora 数据流过程：\n",
    "1. x [140, 1433]\n",
    "2. y [140, 7]\n",
    "3. allx [1708, 1433]\n",
    "4. ally [1708, 7]\n",
    "5. tx [1000, 1433];  # 代表大量孤立点;\n",
    "6. ty [1000, 7];  # 代表大量孤立点;\n",
    "7. graph [2708, ~len] --> edge_index [2, 10858];\n",
    "8. test_index [1000]\n",
    "\n",
    "\n",
    "process(self)\n",
    "1. train_idx [140], val_idx [500] , test_idx [1000]\n",
    "2. x被重新赋值； x = concat(allx, tx)  [2708]\n",
    "3. y被重新赋值； y = concat(ally, ty)  [2708]\n",
    "4. train_mask， val_mask, test_mask 分别就是一个binary vector;\n",
    "5. 将 graph --> edge_indx\n",
    "6. 三元组打包放到Data对象[x, y, edge_index]\n",
    "7. 然后再加上 train_mask， val_mask, test_mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv, ChebConv  # noqa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/qibo/all_project/Graph反欺诈/PYG/raw/' + 'ind.cora.'\n",
    "x_path = path + 'x'\n",
    "y_path = path + 'y'\n",
    "all_x_path = path + 'allx'\n",
    "all_y_path = path + 'ally'\n",
    "tx_path = path + 'tx'\n",
    "ty_path = path + 'ty'\n",
    "graph_path = path + 'graph'\n",
    "test_path = path + 'test.index'\n",
    "\n",
    "with open(x_path, 'rb') as f:\n",
    "    x = pickle.load(f, encoding='latin1')    \n",
    "\n",
    "with open(y_path, 'rb') as f:\n",
    "    y = pickle.load(f, encoding='latin1')    \n",
    "\n",
    "with open(all_x_path, 'rb') as f:\n",
    "    allx = pickle.load(f, encoding='latin1')    \n",
    "\n",
    "with open(all_y_path, 'rb') as f:\n",
    "    ally = pickle.load(f, encoding='latin1')    \n",
    "\n",
    "with open(tx_path, 'rb') as f:\n",
    "    tx = pickle.load(f, encoding='latin1')    \n",
    "\n",
    "with open(ty_path, 'rb') as f:\n",
    "    ty = pickle.load(f, encoding='latin1')    \n",
    "\n",
    "with open(graph_path, 'rb') as f:\n",
    "    graph = pickle.load(f, encoding='latin1')    \n",
    "\n",
    "\n",
    "def parse_txt_array(src, sep=None, start=0, end=None, dtype=None, device=None):\n",
    "    src = [[float(x) for x in line.split(sep)[start:end]] for line in src]\n",
    "    src = torch.tensor(src, dtype=dtype).squeeze()\n",
    "    return src\n",
    "def read_txt_array(path, sep=None, start=0, end=None, dtype=None, device=None):\n",
    "    with open(path, 'r') as f:\n",
    "        src = f.read().split('\\n')[:-1]\n",
    "    return parse_txt_array(src, sep, start, end, dtype, device)\n",
    "\n",
    "test_idx = read_txt_array(test_path, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='./', name='Cora')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch_geometric.datasets.planetoid.Planetoid,\n",
       " torch_geometric.data.in_memory_dataset.InMemoryDataset,\n",
       " torch_geometric.data.dataset.Dataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " object)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Planetoid.__mro__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.6071, Val: 0.3980, Test: 0.3990\n",
      "Epoch: 002, Train: 0.7143, Val: 0.4580, Test: 0.4600\n",
      "Epoch: 003, Train: 0.7429, Val: 0.4900, Test: 0.4880\n",
      "Epoch: 004, Train: 0.7714, Val: 0.4960, Test: 0.4970\n",
      "Epoch: 005, Train: 0.7714, Val: 0.5060, Test: 0.4910\n",
      "Epoch: 006, Train: 0.8000, Val: 0.5060, Test: 0.4910\n",
      "Epoch: 007, Train: 0.8357, Val: 0.5260, Test: 0.5170\n",
      "Epoch: 008, Train: 0.8643, Val: 0.5480, Test: 0.5540\n",
      "Epoch: 009, Train: 0.8857, Val: 0.5820, Test: 0.5770\n",
      "Epoch: 010, Train: 0.9071, Val: 0.6160, Test: 0.6180\n",
      "Epoch: 011, Train: 0.9500, Val: 0.6260, Test: 0.6540\n",
      "Epoch: 012, Train: 0.9643, Val: 0.6660, Test: 0.6780\n",
      "Epoch: 013, Train: 0.9643, Val: 0.6960, Test: 0.7130\n",
      "Epoch: 014, Train: 0.9857, Val: 0.7080, Test: 0.7300\n",
      "Epoch: 015, Train: 0.9929, Val: 0.7140, Test: 0.7430\n",
      "Epoch: 016, Train: 0.9929, Val: 0.7340, Test: 0.7550\n",
      "Epoch: 017, Train: 0.9929, Val: 0.7420, Test: 0.7640\n",
      "Epoch: 018, Train: 0.9929, Val: 0.7440, Test: 0.7660\n",
      "Epoch: 019, Train: 0.9929, Val: 0.7540, Test: 0.7690\n",
      "Epoch: 020, Train: 0.9929, Val: 0.7620, Test: 0.7730\n",
      "Epoch: 021, Train: 0.9929, Val: 0.7660, Test: 0.7820\n",
      "Epoch: 022, Train: 0.9929, Val: 0.7720, Test: 0.7840\n",
      "Epoch: 023, Train: 0.9929, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 024, Train: 0.9929, Val: 0.7800, Test: 0.7840\n",
      "Epoch: 025, Train: 0.9929, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 026, Train: 0.9929, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 027, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 028, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 029, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 030, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 031, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 032, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 033, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 034, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 035, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 036, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 037, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 038, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 039, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 040, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 041, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 042, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 043, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 044, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 045, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 046, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 047, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 048, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 049, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 050, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 051, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 052, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 053, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 054, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 055, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 056, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 057, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 058, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 059, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 060, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 061, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 062, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 063, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 064, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 065, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 066, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 067, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 068, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 069, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 070, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 071, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 072, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 073, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 074, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 075, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 076, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 077, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 078, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 079, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 080, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 081, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 082, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 083, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 084, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 085, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 086, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 087, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 088, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 089, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 090, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 091, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 092, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 093, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 094, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 095, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 096, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 097, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 098, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 099, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 100, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 101, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 102, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 103, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 104, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 105, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 106, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 107, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 108, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 109, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 110, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 111, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 112, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 113, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 114, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 115, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 116, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 117, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 118, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 119, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 120, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 121, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 122, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 123, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 124, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 125, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 126, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 127, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 128, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 129, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 130, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 131, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 132, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 133, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 134, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 135, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 136, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 137, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 138, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 139, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 140, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 141, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 142, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 143, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 144, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 145, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 146, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 147, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 148, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 149, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 150, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 151, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 152, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 153, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 154, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 155, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 156, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 157, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 158, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 159, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 160, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 161, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 162, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 163, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 164, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 165, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 166, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 167, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 168, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 169, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 170, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 171, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 172, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 173, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 174, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 175, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 176, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 177, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 178, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 179, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 180, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 181, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 182, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 183, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 184, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 185, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 186, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 187, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 188, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 189, Train: 1.0000, Val: 0.7820, Test: 0.7880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 191, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 192, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 193, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 194, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 195, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 196, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 197, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 198, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 199, Train: 1.0000, Val: 0.7820, Test: 0.7880\n",
      "Epoch: 200, Train: 1.0000, Val: 0.7820, Test: 0.7880\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_features, 16, cached=True)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes, cached=True)\n",
    "        # self.conv1 = ChebConv(data.num_features, 16, K=2)\n",
    "        # self.conv2 = ChebConv(16, data.num_features, K=2)\n",
    "\n",
    "    def forward(self):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1, 201):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    print(log.format(epoch, train_acc, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
