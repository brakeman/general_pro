{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adj_list(path_array):\n",
    "    '''\n",
    "    para: path_array: df.values with shape:[nodes, 2]\n",
    "    return: adj_list, userid2idx, node_degree\n",
    "    '''\n",
    "    G=nx.Graph()\n",
    "    G.add_edges_from(path_array)\n",
    "    adj_list = G.adjacency_list()\n",
    "    userid2idx = {j:i for i,j in enumerate(G.nodes())}\n",
    "    adj_list = construct_adj_list(G, userid2idx, max_degree=25)\n",
    "    return G, adj_list, userid2idx, G.degree(), G.edges()\n",
    "\n",
    "def make_fake_feat(nodes, emb_size):\n",
    "    '''\n",
    "    para: nodes: a list of user_id;\n",
    "    return: fake fatures of all nodes;'''\n",
    "    features = np.ones(shape = (len(nodes), emb_size))\n",
    "    labels = np.ones(len(nodes))\n",
    "    return features, labels\n",
    "\n",
    "def construct_adj_list(G, id2idx, max_degree):\n",
    "    adj_list = len(id2idx)*np.ones((len(id2idx)+1, max_degree))\n",
    "    for nodeid in G.nodes():\n",
    "        neighbors = np.array([id2idx[neighbor] for neighbor in G.neighbors(nodeid)])\n",
    "        if len(neighbors) == 0:\n",
    "            continue\n",
    "        if len(neighbors) > max_degree:\n",
    "            neighbors = np.random.choice(neighbors, max_degree, replace=False)\n",
    "        elif len(neighbors) < max_degree:\n",
    "            neighbors = np.random.choice(neighbors, max_degree, replace=True)\n",
    "        adj_list[id2idx[nodeid], :] = neighbors\n",
    "    return adj_list.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = './data/col_4.csv'\n",
    "df = pd.read_csv(data, header=None)\n",
    "# 4元素\n",
    "G, adj_list, userid2idx, degree, edges = make_adj_list(df.iloc[:,:2].values)\n",
    "fake_feats, fake_lables = make_fake_feat(G.nodes(), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/Users/cashbus/Documents/工作/任务5-图/graphsage-master/qb_graph_sage/aggregator') \n",
    "sys.path.append('/Users/cashbus/Documents/工作/任务5-图/graphsage-master/qb_graph_sage/sampler')\n",
    "# sys.path.append('/Users/cashbus/Documents/工作/任务5-图/graphsage-master/qb_graph_sage/minibatch')\n",
    "\n",
    "from aggregator import MeanAggregator\n",
    "from sampler import LayerNeighborSampler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage_supervised:\n",
    "    def __init__(self, features, edges, adj_list, labels, degrees, id_map, bs, neg_samples):\n",
    "        self.features = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False)\n",
    "        self.adj_list = adj_list\n",
    "        self.degrees = degrees  # a list of all nodes'degree; same order with id2nodes;\n",
    "        self.sampler = LayerNeighborSampler(adj_list)\n",
    "        self.agg = MeanAggregator()\n",
    "        self.emb_size = features.shape[1]\n",
    "        self.batch_size = bs\n",
    "        self.neg_samples = neg_samples\n",
    "        self.id2idx = id_map\n",
    "        self.edges = edges\n",
    "        self.labels = labels\n",
    "\n",
    "    def sample(self, inputs, layer_infos, batch_size=None):\n",
    "        \"\"\" Sample neighbors to be the supportive fields for multi-layer convolutions.\n",
    "\n",
    "        Args:\n",
    "            layer_infos: [3, 2] # 第一层每个node采样3个， 第二层每个node采样2个\n",
    "            inputs: batch nodes idx; # [bs]\n",
    "            batch_size: the number of inputs (different for batch inputs and negative samples).\n",
    "        \"\"\"\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        samples = [inputs]\n",
    "        support_size = 1\n",
    "        support_sizes = [support_size]\n",
    "        for k in range(len(layer_infos)):\n",
    "            t = len(layer_infos) - k - 1\n",
    "            support_size *= layer_infos[t]\n",
    "            # samples[0] -- layers[1]\n",
    "            # samples[1] -- layers[0]\n",
    "            node = self.sampler.build((samples[k], layer_infos[t]))  # 从该layer nodes idx 关联到全局 adj_mat, 找到该层指定的邻居数；\n",
    "            # 全域adj【num_nodes, max_sample_num], 如果不足，那么重复采样;\n",
    "            # [layer_nodes, max_num_neighs];\n",
    "            samples.append(tf.cast(tf.reshape(node, [support_size * batch_size]), tf.int32)) #emb_lookup 必须是int\n",
    "            support_sizes.append(support_size)\n",
    "\n",
    "        # samples: [bs*support_sizes[0], bs*support_sizes[1], bs*support_sizes[2], ...];\n",
    "        # samples: [5个点， 10个点， 30个点];\n",
    "        # layer_infos: [1, 2, 3];\n",
    "        # support_sizes: [1, 2, 6];\n",
    "        return samples, support_sizes\n",
    "\n",
    "    def aggregate(self, samples, input_features, num_samples, support_sizes, batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        # [5个点的emb, 10个点的emb, 30个点的emb]\n",
    "        hidden = [tf.nn.embedding_lookup(self.features, node_samples) for node_samples in samples]\n",
    "        # [3, 2]\n",
    "        for layer in range(len(num_samples)):\n",
    "            next_hidden = []\n",
    "            # layer = 0, hop = 0, self_feats = 5个点emb, neigh_feats = [5, 2, emb], h = 5个点emb ];\n",
    "            #            hop = 1, self_feats = 10个点emb, neigh_feats = [10, 3, emb], h = 10个点emb];\n",
    "            # 更新 hidden = next_hidden;\n",
    "            # layer = 1, hop = 0, self_feats = 新点5个点emb; neigh_feats = [5, 2, emb], h = 新点5个点emb;\n",
    "            for hop in range(len(num_samples) - layer):\n",
    "                neigh_dims = [batch_size * support_sizes[hop],\n",
    "                              num_samples[len(num_samples) - hop - 1],\n",
    "                              self.emb_size]\n",
    "                self_feats = hidden[hop]\n",
    "                neigh_feats = tf.reshape(hidden[hop + 1], neigh_dims)\n",
    "                h = self.agg.bulid(self_feats, neigh_feats)\n",
    "                next_hidden.append(h)\n",
    "            hidden = next_hidden\n",
    "        return hidden[0]  # [bs, new_emb];\n",
    "\n",
    "    \n",
    "    def clf_loss(self, outputs2, labels):\n",
    "        return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=outputs2, labels=labels))\n",
    "    \n",
    "    def nce_loss(self, inputs1, inputs2, neg_samples, neg_sample_weights):\n",
    "        aff = tf.reduce_sum(inputs1 * inputs2, axis=1)\n",
    "        neg_aff = tf.matmul(inputs1, tf.transpose(neg_samples))\n",
    "        true_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(aff), logits=aff)\n",
    "        negative_xent = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(neg_aff), logits=neg_aff)\n",
    "        loss = tf.reduce_sum(true_xent) + neg_sample_weights * tf.reduce_sum(negative_xent)\n",
    "        print('aff:{}\\nneg_aff:{}\\nloss:{}'.format(aff, neg_aff, loss))\n",
    "        return loss\n",
    "\n",
    "    def batch_feed_dict(self, batch_edges):\n",
    "        batch1 = []\n",
    "        batch2 = []\n",
    "        for node1, node2 in batch_edges:\n",
    "            batch1.append(self.id2idx[node1])\n",
    "            batch2.append(self.id2idx[node2])\n",
    "        return batch1, batch2, len(batch_edges)\n",
    "\n",
    "    def get_ph(self):\n",
    "        inp1_ph = tf.placeholder(tf.int32, shape=(None))\n",
    "        inp2_ph = tf.placeholder(tf.int32, shape=(None))\n",
    "        label_ph = tf.placeholder(tf.float32, shape=(None))\n",
    "        batch_size_ph = tf.placeholder(tf.int32)\n",
    "        return inp1_ph, inp2_ph, label_ph, batch_size_ph\n",
    "\n",
    "    def main(self, settings=[(2,0.01)], layer_infos=[3, 2], neg_sample_size=4):\n",
    "        '''function for graph, training, eval\n",
    "        :layer_infos: num_neighs for each layer;\n",
    "        :setting: training settting (epoch, lr);\n",
    "        '''\n",
    "        RESULT = []\n",
    "        for (num_epochs, lr) in settings:\n",
    "            train_loss_list, test_loss_list = [], []\n",
    "            # tf.reset_default_graph()\n",
    "            gb_step = tf.Variable(initial_value=0, trainable=False)\n",
    "            inputs1, inputs2, labels, batch_size = self.get_ph()  # [bs], [bs];\n",
    "            samples1, support_sizes1 = self.sample(inputs1, layer_infos)  # [bs=5个点， 10个点， 30个点], [1, 2, 6] when layer_infos = [3, 2];\n",
    "            samples2, support_sizes2 = self.sample(inputs2, layer_infos)  # [bs=5个点， 10个点， 30个点], [1, 2, 6] when layer_infos = [3, 2];\n",
    "\n",
    "            outputs1 = self.aggregate(samples1, [self.features], layer_infos, support_sizes1)  # [bs, emb]\n",
    "            outputs2 = self.aggregate(samples2, [self.features], layer_infos, support_sizes2)  # [bs, emb]\n",
    "\n",
    "            tmp = tf.reshape(tf.cast(inputs2, dtype=tf.int64), [self.batch_size, 1])\n",
    "            neg_samples, _, _ = (tf.nn.fixed_unigram_candidate_sampler(\n",
    "                true_classes=tmp, num_true=1,\n",
    "                num_sampled=neg_sample_size,\n",
    "                unique=False,\n",
    "                range_max=len(self.degrees), distortion=0.75,\n",
    "                unigrams=list(self.degrees.values())))\n",
    "\n",
    "            neg_samples = tf.cast(neg_samples, tf.int32)\n",
    "            neg_samples, neg_support_sizes = self.sample(neg_samples, layer_infos, batch_size=neg_sample_size)\n",
    "            neg_outputs = self.aggregate(neg_samples, [self.features], layer_infos, neg_support_sizes, batch_size=neg_sample_size)\n",
    "            outputs2 = tf.layers.dense(outputs2, units=1)\n",
    "            clf_loss = self.clf_loss(outputs2, labels)\n",
    "            loss = self.nce_loss(outputs1, outputs2, neg_outputs, 1)\n",
    "            optimizer = tf.contrib.layers.optimize_loss(loss=clf_loss, learning_rate=lr, optimizer='Adam', global_step=gb_step)\n",
    "\n",
    "            ########## graph finish, training start############\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                epoch = 0\n",
    "                while epoch < num_epochs:\n",
    "                    epoch += 1\n",
    "                    batch_idx = 0\n",
    "                    minibatch = EdgeMinibatchIterSupervised(self.id2idx, self.adj_list, self.edges, self.batch_size, self.labels)\n",
    "                    while not minibatch.end():\n",
    "                        batch_idx += 1\n",
    "                        inp1, inp2, batch_label, temp_bs = minibatch.next_minibatch_feed_dict()\n",
    "                        sess.run(optimizer, feed_dict={inputs1: inp1, inputs2: inp2, labels:batch_label, batch_size: temp_bs})\n",
    "                        if batch_idx % 50 == 0:\n",
    "                            print('batch_idx_for_training:{}'.format(batch_idx))\n",
    "#                             print('temp nce loss:{}'.format(sess.run(loss, feed_dict={inputs1:Val_inp1, inputs2:Val_inp2, batch_size:Val_bs})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qb_sage = GraphSage_supervised(fake_feats, edges, adj_list, fake_lables, degree, userid2idx, 200, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aff:Tensor(\"Sum_3:0\", shape=(200,), dtype=float32)\n",
      "neg_aff:Tensor(\"MatMul_1:0\", shape=(200, 4), dtype=float32)\n",
      "loss:Tensor(\"add_1:0\", shape=(), dtype=float32)\n",
      "batch_idx_for_training:50\n",
      "batch_idx_for_training:100\n",
      "batch_idx_for_training:150\n",
      "batch_idx_for_training:200\n",
      "batch_idx_for_training:250\n",
      "batch_idx_for_training:300\n",
      "batch_idx_for_training:350\n",
      "batch_idx_for_training:400\n",
      "batch_idx_for_training:450\n",
      "batch_idx_for_training:500\n",
      "batch_idx_for_training:550\n",
      "batch_idx_for_training:600\n",
      "batch_idx_for_training:650\n",
      "batch_idx_for_training:700\n",
      "batch_idx_for_training:750\n",
      "batch_idx_for_training:800\n",
      "batch_idx_for_training:850\n",
      "batch_idx_for_training:900\n",
      "batch_idx_for_training:950\n",
      "batch_idx_for_training:50\n",
      "batch_idx_for_training:100\n",
      "batch_idx_for_training:150\n",
      "batch_idx_for_training:200\n",
      "batch_idx_for_training:250\n",
      "batch_idx_for_training:300\n",
      "batch_idx_for_training:350\n",
      "batch_idx_for_training:400\n",
      "batch_idx_for_training:450\n",
      "batch_idx_for_training:500\n",
      "batch_idx_for_training:550\n",
      "batch_idx_for_training:600\n",
      "batch_idx_for_training:650\n",
      "batch_idx_for_training:700\n",
      "batch_idx_for_training:750\n",
      "batch_idx_for_training:800\n",
      "batch_idx_for_training:850\n",
      "batch_idx_for_training:900\n",
      "batch_idx_for_training:950\n"
     ]
    }
   ],
   "source": [
    "qb_sage.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
