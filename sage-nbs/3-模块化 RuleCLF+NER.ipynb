{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "import collections\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from ClfTemplates import get_all_template_path, sample_template, sample_template_path\n",
    "from ClfUse import showClfResult, get_samples, labeledDataFromJson\n",
    "from Ner_model import get_X_Y, lstm_crf, predict, preprocess\n",
    "from Ner_data_make import create_fake_label, fake2real\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import LSTM, Embedding, SpatialDropout1D,concatenate, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from collections import Iterable \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "lines = []\n",
    "filename = 'paradict_0517.json'\n",
    "\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        lines.append(json.loads(line))\n",
    "    \n",
    "para_dict = lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_templates_df = pd.read_csv('gsm_templates_df.csv')\n",
    "example = gsm_templates_df.sample(n=5).sms.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_similarity(sms, templates):\n",
    "    '''计算一条新sms 与 每个template 相似度;'''\n",
    "    def cos_sim(a, b):\n",
    "        return dot(a, b) / (norm(a) * norm(b))\n",
    "    return [cos_sim(i, sms) for i in templates]\n",
    "\n",
    "\n",
    "def tfIdfVector(corpus):\n",
    "    '''corpus is a list of sentences:\n",
    "    ['This is an example', 'hello world', ...]\n",
    "    '''\n",
    "    vectorizer = CountVectorizer()\n",
    "    transformer = TfidfTransformer()\n",
    "    x = vectorizer.fit_transform(corpus)\n",
    "    tfidf = transformer.fit_transform(x)\n",
    "    return tfidf.toarray()\n",
    "\n",
    "class IndiaSms:\n",
    "    '''\n",
    "    处理1条或多条短信类别及NER；\n",
    "    1. 输入模型储存路径 和 参数储存路径，会重新训练model 并保存复现文件：model, para_dict\n",
    "    例子：\n",
    "    test = IndiaSms()\n",
    "    test.fit(DF, batch_size=32, epochs=1, savePath = 'model_test_01.h5',\n",
    "        validation_split=0.1, paraname='shenmegui')\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, model_path = None, para_dict = None):\n",
    "        if model_path:\n",
    "            self.max_len = para_dict['max_len']\n",
    "            self.max_len_char = para_dict['max_len_char']\n",
    "            self.n_tags = para_dict['n_tags']\n",
    "            self.char2idx = para_dict['char2idx']\n",
    "            self.word2idx = para_dict['word2idx']\n",
    "            self.tag2idx = para_dict['tag2idx']\n",
    "            self.model = self._model_load(model_path)\n",
    "        \n",
    "        \n",
    "    def cls(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def fit(self, DF, batch_size, epochs, \n",
    "            validation_split, paraname, savePath):\n",
    "        '''DF like \n",
    "           # token # lable # sent_id\n",
    "           ############################\n",
    "           # This  # other # send_id_0\n",
    "           # is    # other # send_id_0\n",
    "           # an    # other # send_id_0\n",
    "           # apple # Item  # send_id_0\n",
    "           # Pls   # other # send_id_1\n",
    "           # pay   # other # send_id_1\n",
    "           # RMB   # other # send_id_1\n",
    "           # 100   # Money # send_id_1\n",
    "        '''\n",
    "        X_word, X_char, y, max_len, max_len_char, n_tags, word2idx, char2idx, tag2idx = self._preprocess_for_fit(DF)\n",
    "        model = self.NN_model(word2idx, char2idx, max_len, max_len_char, n_tags)\n",
    "        y_tr = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "\n",
    "        history = model.fit([X_word, np.array(X_char).reshape((len(X_char), max_len, max_len_char))],\n",
    "                 np.array(y_tr),\n",
    "                 batch_size=batch_size, epochs=epochs, validation_split=validation_split)\n",
    "        model.save(savePath)\n",
    "\n",
    "        paras = {}\n",
    "        paras['max_len'] = max_len\n",
    "        paras['max_len_char'] = max_len_char\n",
    "        paras['word2idx'] = word2idx\n",
    "        paras['char2idx'] = char2idx\n",
    "        paras['tag2idx'] = tag2idx\n",
    "        paras['n_tags'] = n_tags\n",
    "        \n",
    "        with open(paraname +'.json','w') as outfile:\n",
    "            json.dump(paras, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "        \n",
    "        self.model = model\n",
    "        self.max_len = max_len\n",
    "        self.max_len_char = max_len_char\n",
    "        self.word2idx = word2idx\n",
    "        self.char2idx = char2idx\n",
    "        self.tag2idx = tag2idx\n",
    "        self.n_tags = n_tags\n",
    "        return \n",
    "            \n",
    "    def predict(self, sms):\n",
    "        '''\n",
    "        Should do self.fit(if u wanna retrain the model) first;\n",
    "        Or give an pretrained model and corresponding para_dict in __init__ function.\n",
    "        \n",
    "        sms type:\n",
    "        [['this', 'is', 'an', 'example']]\n",
    "        '''\n",
    "        idx2tag = {i:w for w,i in self.tag2idx.items()}\n",
    "        X_word = self._get_X_word(sms, self.word2idx, self.max_len)\n",
    "        X_char = self._get_X_char(sms, self.char2idx, self.max_len, self.max_len_char)\n",
    "        y_pred = self.model.predict([X_word, np.array(X_char).reshape((len(X_char),self.max_len, self.max_len_char))])\n",
    "        p = np.argmax(y_pred, axis=-1)\n",
    "        pred = [[idx2tag[i] for i in sent] for sent in p]\n",
    "        seq_len = [len(i) for i in sms]\n",
    "        return [pre[:len_] for pre, len_ in zip(pred, seq_len)]\n",
    "    \n",
    "    \n",
    "    def NN_model(self, word2idx, char2idx,\n",
    "                 max_len, max_len_char, n_tags):\n",
    "        word_in = Input(shape=(max_len,))\n",
    "        n_words = len(word2idx.keys())\n",
    "        n_chars = len(char2idx.keys())\n",
    "        emb_word = Embedding(input_dim=n_words, output_dim=30,\n",
    "                            input_length=max_len, mask_zero=True)(word_in)\n",
    "        char_in = Input(shape=(max_len, max_len_char,))\n",
    "        emb_char = TimeDistributed(Embedding(input_dim = n_chars, output_dim=10,\n",
    "                                            input_length=max_len_char, mask_zero=True))(char_in)\n",
    "\n",
    "        char_enc = TimeDistributed(LSTM(units = 20, return_sequences=False,\n",
    "                                       recurrent_dropout=0.3))(emb_char)\n",
    "        x = concatenate([emb_word, char_enc])\n",
    "        x = SpatialDropout1D(0.3)(x)\n",
    "        main_lstm = Bidirectional(LSTM(units=30,return_sequences=True,\n",
    "                                      recurrent_dropout=0.4))(x)\n",
    "        model = TimeDistributed(Dense(35, activation='relu'))(main_lstm)\n",
    "        crf = CRF(n_tags, learn_mode='marginal')\n",
    "        out = crf(model) # prob\n",
    "        model = Model([word_in, char_in], out)\n",
    "#         from keras.utils.vis_utils import plot_model\n",
    "#         plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "        model.compile(optimizer = 'rmsprop',\n",
    "                          loss = crf_loss,\n",
    "                          metrics=[crf.accuracy])\n",
    "#         model.summary()\n",
    "        return model   \n",
    "\n",
    "\n",
    "    def _model_load(self, path):\n",
    "        model = self.NN_model(self.word2idx, self.char2idx,\n",
    "                 self.max_len, self.max_len_char, self.n_tags)\n",
    "        model.load_weights(path)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def _preprocess_for_fit(self, DF):\n",
    "        '''DF like \n",
    "           # token # lable # sent_id\n",
    "           ############################\n",
    "           # This  # other # send_id_0\n",
    "           # is    # other # send_id_0\n",
    "           # an    # other # send_id_0\n",
    "           # apple # Item  # send_id_0\n",
    "           # Pls   # other # send_id_1\n",
    "           # pay   # other # send_id_1\n",
    "           # RMB   # other # send_id_1\n",
    "           # 100   # Money # send_id_1\n",
    "        '''\n",
    "        max_len = 75\n",
    "        max_len_char = 13\n",
    "        words = list(set(DF['token'].values))\n",
    "        words = list(set([i.lower() for i in words]))\n",
    "        tags = list(set(DF['lable'].values))\n",
    "        n_tags  = len(tags)\n",
    "        grouped = DF.groupby('sent_id').apply(lambda x: [(w.lower(), t) for w,t in zip(x.token.tolist(), x.lable.tolist())])\n",
    "        sentences = [s for s in grouped]\n",
    "        \n",
    "        word2idx = {w: i+2 for i,w in enumerate(words)}\n",
    "        word2idx['UNK']=1\n",
    "        word2idx['PAD']=0\n",
    "        \n",
    "        tag2idx = {t:i for i, t in enumerate(tags)}\n",
    "        \n",
    "        chars = set([w_i for w in words for w_i in w])\n",
    "        char2idx = {c:i+2 for i,c in enumerate(chars)}\n",
    "        char2idx['UNK']=1\n",
    "        char2idx['PAD']=0\n",
    "        \n",
    "        # 句子词序列 转为 句子词序号序列\n",
    "        sentences_X = [[tuple_[0] for tuple_ in sent] for sent in sentences]\n",
    "        X_word = self._get_X_word(sentences_X, word2idx, max_len)\n",
    "        X_char = self._get_X_char(sentences_X, char2idx, max_len, max_len_char)\n",
    "        y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "        y = pad_sequences(maxlen = max_len, sequences=y, \n",
    "                          value=tag2idx['other'], padding='post',truncating='post')\n",
    "        return X_word, X_char, y, max_len, max_len_char, n_tags, word2idx, char2idx, tag2idx\n",
    "    \n",
    "    def _get_X_word(self, sentences, word2idx, max_len):\n",
    "        '''sentences type: [['this','is','an','example']]'''\n",
    "        X_word = [[word2idx.get(w[0], word2idx['UNK']) for w in s] for s in sentences]\n",
    "        X_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx['PAD'],padding='post',truncating='post')\n",
    "        return X_word\n",
    "    \n",
    "    def _get_X_char(self, sentences, char2idx, max_len, max_len_char):\n",
    "        '''sentences type: [['this','is','an','example']]'''\n",
    "        X_char = []\n",
    "        for sentence in sentences:\n",
    "            sent_seq = []\n",
    "            for i in range(max_len):\n",
    "                word_seq = []\n",
    "                for j in range(max_len_char):\n",
    "                    try:\n",
    "                        word_seq.append(char2idx.get(sentence[i][j]))\n",
    "                    except:\n",
    "                        word_seq.append(char2idx.get('PAD'))\n",
    "                sent_seq.append(word_seq)\n",
    "            X_char.append(np.array(sent_seq))\n",
    "        return X_char\n",
    "    \n",
    "\n",
    "class SmsRuleClf:\n",
    "    '''模块功能: 给我一个(组)sms， 能够对其正确分类'''\n",
    "    def __init__(self, labeled_templates_df):\n",
    "        self.labeled_templates_df = labeled_templates_df\n",
    "        self.corpus, self.labels = self._get_template_corpus_labels()\n",
    "        \n",
    "    def _get_template_corpus_labels(self):\n",
    "        corpus, labels = self.labeled_templates_df.sms.tolist(), self.labeled_templates_df.label.tolist()\n",
    "        return corpus, labels\n",
    "\n",
    "    def predict(self, sms):\n",
    "        '''\n",
    "        Input: ['This is an example', 'hello world', ...]\n",
    "        Output: [cls1, cls2, ...]\n",
    "        '''\n",
    "        if isinstance(sms, list) and isinstance(sms[0], str):\n",
    "            template_corpus, template_labels = self.corpus, self.labels\n",
    "            num_sms = len(sms)\n",
    "            for single in sms:\n",
    "                template_corpus.append(single.lower())\n",
    "            all_tfidf = tfIdfVector(template_corpus)\n",
    "            template_tfidf = all_tfidf[:-num_sms]\n",
    "            instances_tfidf = all_tfidf[-num_sms:]\n",
    "            \n",
    "            result = []\n",
    "            for idx, single_sms in enumerate(instances_tfidf):  \n",
    "                cos_score = get_cos_similarity(single_sms, template_tfidf)\n",
    "                max_score = np.max(cos_score)\n",
    "                label = template_labels[np.argmax(cos_score)]\n",
    "                result.append([sms[idx], label, max_score])\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception('''sms type not allowed: should be with type: ['This is an example', 'hello world', ...]''')\n",
    "            \n",
    "def generate_clf_ner(sms, gsm_templates_df, model, para_dict):\n",
    "    '''\n",
    "    sms example: ['successful transfer of rs 1800 to shubham pa using airtel money transfer by - 919892750965.txn id: 1170827753.charges - max 0.65%',\n",
    "                  'thank you for using your citibank debit card 5497xxxxxxxx2902 for rs. 6325 at sbicard-billdesk on 28-sep-18.']\n",
    "    \n",
    "    '''\n",
    "    sms = [i.lower() for i in sms]\n",
    "    sms_ner = [i.split() for i in sms]\n",
    "    \n",
    "    #CLF\n",
    "    clf = SmsRuleClf(gsm_templates_df)\n",
    "    clf_result = clf.predict(example)\n",
    "    \n",
    "    #NER\n",
    "    Ner = IndiaSms(model, para_dict=para_dict)\n",
    "    ner_result = Ner.predict(sms_ner)\n",
    "    \n",
    "    # combine\n",
    "    record = []\n",
    "    for clf_, ner_ in zip(clf_result, ner_result):\n",
    "        ner_res = [{i: clf_[0].split()[j]} for j,i in enumerate(ner_) if i !='other']\n",
    "        record.append([clf_[0], clf_[1], clf_[2], ner_res])\n",
    "    Final_DF = pd.DataFrame.from_records(record)\n",
    "    Final_DF.columns = ['sms', 'class', 'class_score', 'ner_result']\n",
    "    return Final_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = generate_clf_ner(gsm_templates_df.sms.tolist(), gsm_templates_df, 'model_0517.h5', para_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms</th>\n",
       "      <th>class</th>\n",
       "      <th>class_score</th>\n",
       "      <th>ner_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dear customer, payment received for amount of ...</td>\n",
       "      <td>交易流水＿转账</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'银行卡号＿自己': 'for'}, {'金额＿转出': 'xxxx2798'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we are pleased to inform you that your request...</td>\n",
       "      <td>交易流水＿转账</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'账户账号＿自己': 'inform'}, {'金额＿转出': 'for'}, {'金额...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we acknowledge receipt of payment via 10522182...</td>\n",
       "      <td>交易流水＿转账</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'账户账号＿自己': 'of'}, {'金额＿转入': 'for'}, {'日期＿交易时...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dear customer.thank you for your recent paymen...</td>\n",
       "      <td>贷后提醒＿到期提醒</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'账户账号＿自己': 'your'}, {'金额＿转出': 'hdfc'}, {'金额＿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>your a/c no. ******4535 is credited for ₹50.00...</td>\n",
       "      <td>交易流水＿转账</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'账户账号＿自己': '******4535'}, {'金额＿转出': '₹50.00'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sms      class  class_score  \\\n",
       "0  dear customer, payment received for amount of ...    交易流水＿转账          1.0   \n",
       "1  we are pleased to inform you that your request...    交易流水＿转账          1.0   \n",
       "2  we acknowledge receipt of payment via 10522182...    交易流水＿转账          1.0   \n",
       "3  dear customer.thank you for your recent paymen...  贷后提醒＿到期提醒          1.0   \n",
       "4  your a/c no. ******4535 is credited for ₹50.00...    交易流水＿转账          1.0   \n",
       "\n",
       "                                          ner_result  \n",
       "0        [{'银行卡号＿自己': 'for'}, {'金额＿转出': 'xxxx2798'}]  \n",
       "1  [{'账户账号＿自己': 'inform'}, {'金额＿转出': 'for'}, {'金额...  \n",
       "2  [{'账户账号＿自己': 'of'}, {'金额＿转入': 'for'}, {'日期＿交易时...  \n",
       "3  [{'账户账号＿自己': 'your'}, {'金额＿转出': 'hdfc'}, {'金额＿...  \n",
       "4  [{'账户账号＿自己': '******4535'}, {'金额＿转出': '₹50.00'...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
