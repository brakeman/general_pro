{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "graph_sage 实验；承接实验A-实验B\n",
    "实验A 得出少样本半监督结论\n",
    "实验B GCN 效果不佳（0.5左右 F1 + 太慢 + 非对抗）\n",
    "因为思想一样，但是更快, 所以在Sage上做优化；\n",
    "GraghSage -- 结合PinSage;\n",
    "\n",
    "v0: \n",
    "v1: 损失函数 无监督（高内聚低耦合） + 监督分类；\n",
    "v2: 全局依赖 相似度， 相似度依赖 emb, emb 我用的tf-idf, 改为bert finetuning;\n",
    "\n",
    "应该做一个 隐藏层实验： 实验说明 sms 相似度确实在提升；只有依赖这个，才能保证高测试集F1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-2abcc2124176>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-2abcc2124176>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    1. 我只利用了 tf-idf 特征 ; --> bert;\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "这是版本1 ;\n",
    "问题在于 ;\n",
    "1. 我只利用了 tf-idf 特征 ; --> bert;\n",
    "2. 尽管利用了邻居和自己，但是邻居的定义存在问题(hard rank);--> 可以用我的weighted * mask;\n",
    "3. 损失函数是纯监督 --> 其实可以纳入负采样无监督；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(G, output, label_idx, idx2label):\n",
    "    SF = nn.Softmax(dim=-1)\n",
    "    score, pred = SF(output).max(dim=-1)\n",
    "    sms = [G.node[i]['text'] for i in label_idx]\n",
    "    df = pd.DataFrame()\n",
    "    df['sms']= sms\n",
    "    df['score']=score.data\n",
    "    df['pred']=[idx2label[i] for i in pred.numpy()]\n",
    "    df['label']=[g4.node[i]['label'] for i in label_idx]\n",
    "    return df\n",
    "\n",
    "def get_clf_prediction(model, X_word, X_char, y):\n",
    "    print('X shape:{}, y_shape:{}'.format(len(X_word), len(y)))\n",
    "    y_pred = model.predict([X_word,\n",
    "                            np.array(X_char).reshape((len(X_char),\n",
    "                                                         max_len, max_len_char))])\n",
    "    pred = [idx2label[i] for i in np.argmax(y_pred,axis=-1)]\n",
    "    true = [idx2label[i] for i in y]\n",
    "    return true, pred\n",
    "\n",
    "def get_pr(Res, cls):\n",
    "    #某类 正确识别数量/该类 总识别数量\n",
    "    tmp  = Res[Res.true == cls]\n",
    "    a = sum(tmp.true.values == tmp.pred.values)\n",
    "    b = Res[Res.pred == cls].shape[0]\n",
    "    pr = a/b\n",
    "    return pr\n",
    "\n",
    "def get_rc(Res, cls):\n",
    "    #某类 正确识别数量/该类 总数量\n",
    "    tmp  = Res[Res.true == cls]\n",
    "    support = tmp.shape[0]\n",
    "    a = sum(tmp.true.values == tmp.pred.values)\n",
    "    b = Res[Res.true == cls].shape[0]\n",
    "    rc = a/b\n",
    "    return rc,support\n",
    "\n",
    "def get_f1(pr, rc):\n",
    "    # f1 = (2*pr*rc)/(pr+rc)\n",
    "    f1 = (2*pr*rc)/(pr+rc)\n",
    "    return f1\n",
    "\n",
    "def evaluate(Res, cls):\n",
    "    pr = get_pr(Res, cls)\n",
    "    rc, support = get_rc(Res, cls)\n",
    "    f1 = get_f1(pr, rc)\n",
    "    return [cls, pr, rc, f1, support]\n",
    "\n",
    "def Final_evalu(z):\n",
    "    Res = pd.DataFrame(columns=['true', 'pred'])\n",
    "    true, pred = z.label.tolist(), z.pred.tolist()\n",
    "    Res['true'] = true\n",
    "    Res['pred'] = pred\n",
    "    entitys = Res.true.unique()\n",
    "    records = []\n",
    "    for i in entitys:\n",
    "        tmp = evaluate(Res, i)\n",
    "        records.append(tmp)\n",
    "    record = pd.DataFrame.from_records(records)\n",
    "    record.columns =['cls','精确率','召回率', 'F1', 'support']\n",
    "    record = record.set_index('cls')\n",
    "    record = record.sort_index()   \n",
    "    return record\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "# from graphviz import Digraph\n",
    "from torch.nn import init\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "# from collections import defaultdict\n",
    "# from ClfTemplates import get_all_template_path, sample_template, sample_template_path\n",
    "# from ClfUse import showClfResult, get_samples, labeledDataFromJson\n",
    "# from Ner_model import get_X_Y, lstm_crf, predict, preprocess\n",
    "# from Ner_data_make import create_fake_label, fake2real\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers import LSTM, Embedding, SpatialDropout1D,concatenate, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.losses import crf_loss\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from collections import Iterable \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cos_similarity(sms, templates):\n",
    "    '''计算一条新sms 与 每个 template 相似度;'''\n",
    "    def cos_sim(a, b):\n",
    "        return dot(a, b) / (norm(a) * norm(b))\n",
    "    return [cos_sim(i, sms) for i in templates]\n",
    "\n",
    "\n",
    "def tfIdfVector(corpus):\n",
    "    '''\n",
    "    corpus is a list of sentences:\n",
    "    ['This is an example', 'hello world', ...]\n",
    "    '''\n",
    "    vectorizer = CountVectorizer()\n",
    "    transformer = TfidfTransformer()\n",
    "    x = vectorizer.fit_transform(corpus)\n",
    "    tfidf = transformer.fit_transform(x)\n",
    "    return tfidf.toarray()\n",
    "\n",
    "def GraphBuild(template_df, new_sms, K=10, thresh=None):\n",
    "    '''\n",
    "    之前版本没考虑到采样需要全图遍历；故在入图过程就把K_nerghs当作点属性加进去；\n",
    "    new_sms: test samples, df type;\n",
    "    \n",
    "    '''\n",
    "    def findNeighsK(arr, K):\n",
    "        K+=1 #入图时neighs 不包括自己，故这里先加一个保证去除自己之后仍然有K neighs;\n",
    "        idx=np.argpartition(arr, -K)[-K:]\n",
    "        return idx\n",
    "\n",
    "    corpus, labels = template_df.sms.tolist(), template_df.cls.tolist()\n",
    "    new_labels = new_sms.cls.tolist()\n",
    "    new_sms = new_sms.sms.tolist()\n",
    "    num_sms = len(new_sms)\n",
    "    for single in new_sms:\n",
    "        corpus.append(single.lower())\n",
    "    for single in new_labels:\n",
    "        labels.append(single)\n",
    "    all_tfidf = tfIdfVector(corpus)\n",
    "    adj_mat = cosine_similarity(all_tfidf)\n",
    "    print('adj mat shape:{}'.format(adj_mat.shape))\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(len(adj_mat)))\n",
    "    for i in range(len(G.nodes())):\n",
    "        G.node[i]['vec'] =  all_tfidf[i]\n",
    "        G.node[i]['text'] = corpus[i]\n",
    "        G.node[i]['label'] = labels[i]\n",
    "        G.node[i]['neighs_k'] = set(findNeighsK(adj_mat[i], K)) - set([i])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_templates_df = pd.read_csv('gsm_templates_df.csv')\n",
    "gsm_templates_df.dropna(inplace=True)\n",
    "gsm_templates_df.columns = ['sms', 'cls']\n",
    "test_df = gsm_templates_df.sample(100)\n",
    "train_df = gsm_templates_df[~gsm_templates_df.index.isin(test_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj mat shape:(1195, 1195)\n"
     ]
    }
   ],
   "source": [
    "g4 = GraphBuild(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Set of modules for aggregating embeddings of neighbors.\n",
    "\"\"\"\n",
    "class MeanAggregator_QBv2(nn.Module):\n",
    "    \"\"\"\n",
    "    adjMat@embMat: [bs, uniqueNode]@[uniqueNode, F]=[bs, F]\n",
    "    \n",
    "    要改，之前没注意每次采样全图遍历；\n",
    "    如无必要，不要让一个实例挂一个毒瘤: G (20G内存)\n",
    "    \"\"\"\n",
    "    def __init__(self, features, cuda=False, gcn=False): \n",
    "        \"\"\"\n",
    "        Initializes the aggregator for a specific graph.\n",
    "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
    "        cuda -- whether to use GPU;\n",
    "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
    "        \"\"\"\n",
    "        super(MeanAggregator_QBv2, self).__init__()\n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "        \n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        \"\"\"\n",
    "        nodes --- list of nodes in a batch, [bs]\n",
    "        to_neighs --- list of sets, \n",
    "                      each set is the set of neighbors for node in batch\n",
    "        num_sample --- number of neighbors to sample. \n",
    "        \"\"\"\n",
    "        _set = set\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, num_sample))\n",
    "                           if len(to_neigh) >= num_sample else to_neigh \n",
    "                           for to_neigh in to_neighs]\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "    \n",
    "        # 纳入target idx;\n",
    "        if self.gcn:\n",
    "            samp_neighs = [set.union(samp_neigh, set([nodes[i]])) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "        unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        mask = mask.div(num_neigh)\n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        return to_feats\n",
    "\n",
    "    \n",
    "class Encoder_QBv2(nn.Module):\n",
    "    \"\"\"\n",
    "    ([cls, 2f]@[2f, bs]).T = [bs, cls] \n",
    "    Encodes a node's using 'convolutional' GraphSage approach\n",
    "    \"\"\"\n",
    "    def __init__(self, features, feature_dim, \n",
    "            embed_dim, adj_lists, aggregator,\n",
    "            num_sample=10,\n",
    "            base_model=None, gcn=False, cuda=False, \n",
    "            feature_transform=False): \n",
    "        super(Encoder_QBv2, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        self.gcn = gcn\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(\n",
    "                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes     -- list of nodes; [bs]\n",
    "        operations: 已知self_feat: [bs, F]\n",
    "                    再agg_feat: [bs, F]\n",
    "                    然后concat([agg_feat, self_feat]): [bs, 2F]\n",
    "                    然后W:[F2, 2F] \n",
    "                    relu(W @[bs,2F].T) : [F2, bs] \n",
    "        \"\"\"\n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], \n",
    "                self.num_sample)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        else:\n",
    "            combined = neigh_feats\n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        return combined\n",
    "\n",
    "\n",
    "\n",
    "class SupervisedGraphSage_QB(nn.Module):\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage_QB, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "        self.loss = nn.NLLLoss(reduce=False)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, nodes, softmax=False):\n",
    "        embeds = self.enc(nodes)\n",
    "        scores = self.weight.mm(embeds)\n",
    "        if softmax:\n",
    "            return self.softmax(scores.t())\n",
    "        return scores.t()\n",
    "\n",
    "    def loss_softmax_mask(self, nodes, labels, mask=None):\n",
    "        '''\n",
    "        nodes shape: [N];\n",
    "        labels shape: [N];\n",
    "        mask shape: [N];\n",
    "        '''\n",
    "        if mask is None:\n",
    "            mask = torch.ones(len(labels))\n",
    "        logits = self.forward(nodes)\n",
    "        A_soft = self.logsoftmax(logits)\n",
    "        output = self.loss(A_soft, labels.squeeze())\n",
    "        loss = output*mask.type(torch.float)\n",
    "        loss = sum(loss)/sum(mask)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "def result(G, output, label_idx, idx2label):\n",
    "    score, pred = output.max(dim=-1)\n",
    "    sms = [G.node[i]['text'] for i in label_idx]\n",
    "    df = pd.DataFrame()\n",
    "    df['sms']= sms\n",
    "    df['score']=score.data\n",
    "    df['pred']=[idx2label[i] for i in pred.numpy()]\n",
    "    df['label']=[g4.node[i]['label'] for i in label_idx]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_india_sms(G):\n",
    "    feat_data = [G.node[i]['vec'] for i in G.nodes]\n",
    "    labels = [G.node[i]['label'] for i in G.nodes]\n",
    "    adj_lists = [G.node[i]['neighs_k'] for i in G.nodes]\n",
    "    return feat_data, labels, adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "feat_data, labels, adj_lists = load_india_sms(g4)\n",
    "idx2label = {i:j for i, j in enumerate(np.unique(labels))}\n",
    "label2idx = {j:i for i,j in idx2label.items()}\n",
    "labels = np.array([label2idx[i] for i in labels])\n",
    "num_cls = len(np.unique(labels))\n",
    "features = nn.Embedding(len(feat_data), len(feat_data[0]))\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "node_F = len(feat_data[0])\n",
    "agg1 = MeanAggregator_QBv2(features, cuda=False, gcn=True)\n",
    "enc1 = Encoder_QBv2(features, node_F, 128, adj_lists, agg1, num_sample=5, gcn=True, cuda=False)\n",
    "agg2 = MeanAggregator_QBv2(lambda nodes : enc1(nodes).t(), cuda=False, gcn=True)\n",
    "enc2 = Encoder_QBv2(lambda nodes : enc1(nodes).t(), \n",
    "               enc1.embed_dim, node_F, adj_lists, agg2, num_sample=5, gcn=True, cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO\n",
      "batch:0; train_loss:2.8288917541503906; val_loss:2.82924747467041\n",
      "batch:100; train_loss:1.558699369430542; val_loss:1.7151776552200317\n",
      "batch:200; train_loss:1.4363244771957397; val_loss:1.6016473770141602\n",
      "batch:300; train_loss:1.3642029762268066; val_loss:1.5427944660186768\n",
      "batch:400; train_loss:1.3100275993347168; val_loss:1.4732929468154907\n",
      "batch:500; train_loss:1.2747801542282104; val_loss:1.4334924221038818\n",
      "batch:600; train_loss:1.2323875427246094; val_loss:1.4309425354003906\n",
      "batch:700; train_loss:1.1975066661834717; val_loss:1.3915148973464966\n",
      "batch:800; train_loss:1.166976809501648; val_loss:1.364182710647583\n",
      "batch:900; train_loss:1.1433796882629395; val_loss:1.3438844680786133\n",
      "batch:1000; train_loss:1.1128835678100586; val_loss:1.2868318557739258\n",
      "batch:1100; train_loss:1.0935643911361694; val_loss:1.288069248199463\n",
      "batch:1200; train_loss:1.0736873149871826; val_loss:1.278058409690857\n",
      "batch:1300; train_loss:1.0496153831481934; val_loss:1.232122778892517\n",
      "batch:1400; train_loss:1.02336585521698; val_loss:1.1955146789550781\n",
      "batch:1500; train_loss:1.0099104642868042; val_loss:1.1870990991592407\n",
      "batch:1600; train_loss:1.0002264976501465; val_loss:1.1804567575454712\n",
      "batch:1700; train_loss:0.9745403528213501; val_loss:1.1699069738388062\n",
      "batch:1800; train_loss:0.9685786366462708; val_loss:1.1775332689285278\n",
      "batch:1900; train_loss:0.967136800289154; val_loss:1.1922575235366821\n",
      "batch:2000; train_loss:0.9432513117790222; val_loss:1.1859345436096191\n",
      "batch:2100; train_loss:0.9364541172981262; val_loss:1.1352133750915527\n",
      "batch:2200; train_loss:0.9271320700645447; val_loss:1.1365454196929932\n",
      "batch:2300; train_loss:0.913061261177063; val_loss:1.1652029752731323\n",
      "batch:2400; train_loss:0.9088329076766968; val_loss:1.0971288681030273\n",
      "batch:2500; train_loss:0.8948808312416077; val_loss:1.07942533493042\n",
      "batch:2600; train_loss:0.8897832632064819; val_loss:1.1436055898666382\n",
      "batch:2700; train_loss:0.8884166479110718; val_loss:1.1122761964797974\n",
      "batch:2800; train_loss:0.8691047430038452; val_loss:1.0946784019470215\n",
      "batch:2900; train_loss:0.8617257475852966; val_loss:1.1036744117736816\n",
      "batch:3000; train_loss:0.8497952222824097; val_loss:1.1035218238830566\n",
      "batch:3100; train_loss:0.8479552268981934; val_loss:1.062077283859253\n",
      "batch:3200; train_loss:0.834649384021759; val_loss:1.0913296937942505\n",
      "batch:3300; train_loss:0.8388482928276062; val_loss:1.066300392150879\n",
      "batch:3400; train_loss:0.8253841996192932; val_loss:1.0522637367248535\n",
      "batch:3500; train_loss:0.8281069993972778; val_loss:1.0304337739944458\n",
      "batch:3600; train_loss:0.8265774250030518; val_loss:0.9985288381576538\n",
      "batch:3700; train_loss:0.8221845626831055; val_loss:1.0684399604797363\n",
      "batch:3800; train_loss:0.8041912913322449; val_loss:1.1083396673202515\n",
      "batch:3900; train_loss:0.7984315752983093; val_loss:0.9843488335609436\n",
      "batch:4000; train_loss:0.8031321167945862; val_loss:1.055468201637268\n",
      "batch:4100; train_loss:0.7963896989822388; val_loss:1.0670831203460693\n",
      "batch:4200; train_loss:0.8002397418022156; val_loss:1.0174564123153687\n",
      "batch:4300; train_loss:0.8029927015304565; val_loss:1.0085583925247192\n",
      "batch:4400; train_loss:0.7880682349205017; val_loss:1.046433925628662\n",
      "batch:4500; train_loss:0.7804998159408569; val_loss:0.994835376739502\n",
      "batch:4600; train_loss:0.7752689719200134; val_loss:1.0135496854782104\n",
      "batch:4700; train_loss:0.7640942931175232; val_loss:1.1376850605010986\n",
      "batch:4800; train_loss:0.7682384848594666; val_loss:1.027068853378296\n",
      "batch:4900; train_loss:0.7596699595451355; val_loss:1.0739918947219849\n"
     ]
    }
   ],
   "source": [
    "graphsage = SupervisedGraphSage_QB(num_cls, enc2)\n",
    "#graphsage.cuda()\n",
    "#rand_indices = np.random.permutation(len(node_list))\n",
    "nodes = list(g4.nodes)\n",
    "val = nodes[:50]\n",
    "test_nodes = list(g4.nodes)[-test_df.shape[0]:]\n",
    "train = nodes[50:-len(test_nodes)]\n",
    "optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr = 0.8)\n",
    "times = []\n",
    "Loss = []\n",
    "print('GO')\n",
    "for batch in range(5000):\n",
    "    batch_nodes2 = train[:200]\n",
    "    batch_idx = copy.copy(train[:200])\n",
    "    random.shuffle(train)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    loss = graphsage.loss_softmax_mask(batch_nodes2,\n",
    "            Variable(torch.tensor(labels[np.array(batch_idx)])))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "#     print(graphsage.forward(val, True)[0].sum())\n",
    "    if batch%100 == 0:\n",
    "        train_loss = graphsage.loss_softmax_mask(train,\n",
    "            Variable(torch.tensor(labels[np.array(train)])))\n",
    "        val_loss = graphsage.loss_softmax_mask(val,\n",
    "            Variable(torch.tensor(labels[np.array(val)])))\n",
    "        print('batch:{}; train_loss:{}; val_loss:{}'.format(batch, train_loss.data, val_loss.data))\n",
    "    Loss.append(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>精确率</th>\n",
       "      <th>召回率</th>\n",
       "      <th>F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cls</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms_other</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易流水＿余额</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易流水＿转账</th>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.863309</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>信用卡＿还款提醒</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>信用卡＿逾期警告</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>账号异常＿余额不足</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>账号异常＿卡号冻结</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>账号异常＿扣款失败</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷前申请＿审核拒绝</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷前申请＿审核通过</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷前申请＿申请交互</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷后提醒＿到期提醒</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷后提醒＿成功放款</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷后提醒＿逾期催收</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                精确率       召回率        F1  support\n",
       "cls                                             \n",
       "sms_other  0.000000  0.000000       NaN        3\n",
       "交易流水＿余额         NaN  0.000000       NaN        7\n",
       "交易流水＿转账    0.769231  0.983607  0.863309       61\n",
       "信用卡＿还款提醒   0.800000  1.000000  0.888889        8\n",
       "信用卡＿逾期警告        NaN  0.000000       NaN        1\n",
       "账号异常＿余额不足  0.250000  0.333333  0.285714        3\n",
       "账号异常＿卡号冻结       NaN  0.000000       NaN        2\n",
       "账号异常＿扣款失败  0.666667  0.500000  0.571429        8\n",
       "贷前申请＿审核拒绝       NaN  0.000000       NaN        1\n",
       "贷前申请＿审核通过       NaN  0.000000       NaN        1\n",
       "贷前申请＿申请交互       NaN  0.000000       NaN        2\n",
       "贷后提醒＿到期提醒       NaN  0.000000       NaN        1\n",
       "贷后提醒＿成功放款       NaN  0.000000       NaN        1\n",
       "贷后提醒＿逾期催收  1.000000  1.000000  1.000000        1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_output = graphsage.forward(test_nodes)\n",
    "test_res = result(g4, te_output, test_nodes, idx2label)\n",
    "Final_evalu(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>精确率</th>\n",
       "      <th>召回率</th>\n",
       "      <th>F1</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cls</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms_other</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易流水＿余额</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>交易流水＿转账</th>\n",
       "      <td>0.822850</td>\n",
       "      <td>0.962462</td>\n",
       "      <td>0.887197</td>\n",
       "      <td>666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>信用卡＿申请失败</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>信用卡＿还款提醒</th>\n",
       "      <td>0.619565</td>\n",
       "      <td>0.838235</td>\n",
       "      <td>0.712500</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>信用卡＿逾期警告</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>账号异常＿余额不足</th>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>账号异常＿信用额度不足</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>账号异常＿卡号冻结</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>账号异常＿扣款失败</th>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.554455</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷前申请＿审核拒绝</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷前申请＿审核通过</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷前申请＿申请交互</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷后提醒＿到期提醒</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷后提醒＿成功放款</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>贷后提醒＿逾期催收</th>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  精确率       召回率        F1  support\n",
       "cls                                               \n",
       "sms_other    0.600000  0.078947  0.139535       38\n",
       "交易流水＿余额      0.818182  0.176471  0.290323       51\n",
       "交易流水＿转账      0.822850  0.962462  0.887197      666\n",
       "信用卡＿申请失败          NaN  0.000000       NaN        3\n",
       "信用卡＿还款提醒     0.619565  0.838235  0.712500       68\n",
       "信用卡＿逾期警告     0.333333  0.052632  0.090909       19\n",
       "账号异常＿余额不足    0.693548  0.716667  0.704918       60\n",
       "账号异常＿信用额度不足       NaN  0.000000       NaN        1\n",
       "账号异常＿卡号冻结    1.000000  0.210526  0.347826       19\n",
       "账号异常＿扣款失败    0.595745  0.518519  0.554455       54\n",
       "贷前申请＿审核拒绝         NaN  0.000000       NaN        4\n",
       "贷前申请＿审核通过         NaN  0.000000       NaN        8\n",
       "贷前申请＿申请交互    1.000000  0.100000  0.181818       10\n",
       "贷后提醒＿到期提醒    0.500000  0.238095  0.322581       21\n",
       "贷后提醒＿成功放款         NaN  0.000000       NaN        3\n",
       "贷后提醒＿逾期催收    0.451613  0.700000  0.549020       20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tra_output = graphsage.forward(train)\n",
    "train_res = result(g4, tra_output, train, idx2label)\n",
    "Final_evalu(train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
